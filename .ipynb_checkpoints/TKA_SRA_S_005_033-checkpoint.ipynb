{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54dc0a0",
   "metadata": {},
   "source": [
    "# Architektur Neuronales Netz, Output x_H2 und x_NH3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250c18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe / Bibliotheken\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import normalize as norm\n",
    "from torch import log10\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import max_error\n",
    "# from sklearn.metrics import mean_squared_error as MSE\n",
    "# from sklearn.metrics import mean_absolute_error as MAE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9ebf3",
   "metadata": {},
   "source": [
    "#### Default Datentyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68df48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e5067",
   "metadata": {},
   "source": [
    "#### Erzeugnung des Moduls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bffc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    #Initalisierung der Netzwerk layers\n",
    "    def __init__(self, input_size, hidden1_size, output_size):\n",
    "    \n",
    "        super().__init__() #Referenz zur Base Class (nn.Module)\n",
    "        #Kaskade der Layer\n",
    "        self.linear_afunc_stack = nn.Sequential(\n",
    "            #nn.BatchNorm1d(input_size), # Normalisierung, damit Inputdaten gleiche Größenordnung haben\n",
    "            nn.Linear(input_size, hidden1_size), #Lineare Transformation mit gespeicherten weights und biases\n",
    "            nn.GELU(), #Nicht lineare Aktivierungsfunktion um komplexe nichtlineare Zusammenhänge abzubilden\n",
    "            nn.Linear(hidden1_size, output_size),\n",
    "        )\n",
    "\n",
    "    #Implementierung der Operationen auf Input Daten\n",
    "    def forward(self, x):\n",
    "        out = self.linear_afunc_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9ae53",
   "metadata": {},
   "source": [
    "#### Ausgabe Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_afunc_stack): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=200, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=200, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(5, 200, 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1d6ae",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08ff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #Zahl der Datenpaare die vor einem erneuten Update der Parameter ins Netzt gegeben werden\n",
    "eq_data_file = Path.cwd() / 'data' / 'eq_dataset_x.npz' #Import der GGW Daten\n",
    "\n",
    "res = np.load(eq_data_file)\n",
    "\n",
    "# Bei Speicherung wurden Daten als T, p, x_0 und xi gespeichert\n",
    "# Inputs T, p, x_0[H2,N2,NH3]\n",
    "# Outputs x[H2,N2,NH3]\n",
    "# Umwandlen der np.arrays in torch.tensors zur besseren Arbeit mit PyTorch\n",
    "T = torch.tensor(res['T'])\n",
    "p = torch.tensor(res['p'])\n",
    "x_0 = torch.tensor(res['x_0'])\n",
    "x = torch.tensor(res['x'])\n",
    "\n",
    "#Anpassen der Daten auf gleiche Größenordnung\n",
    "#T = log10(T)\n",
    "# T = T / 850\n",
    "# p = p / 1000\n",
    "\n",
    "\n",
    "# print(T.dtype)\n",
    "# print(xi.dtype)\n",
    "\n",
    "x_input = torch.stack((T, p ,x_0[:,0],x_0[:,1],x_0[:,2]),1)\n",
    "y_output = torch.stack((x[:,0], x[:,2]), 1) # [H2, NH3], dritter Stoffmengenanteil ergibt sich den anderen\n",
    "#print(x_input.size())\n",
    "# print(xi.size())\n",
    "\n",
    "# Split des Datensatzes in Trainings und Testdaten\n",
    "split = 0.8 # Anteil Trainingsdaten\n",
    "\n",
    "x_input_train = x_input[:int(split * len(x_input)), :]\n",
    "y_output_train = y_output[:int(split * len(y_output)), :]\n",
    "x_input_test = x_input[int(split * len(x_input)):, :]\n",
    "y_output_test = y_output[int(split * len(y_output)):, :]\n",
    "\n",
    "# Preprocessing Normalisierung der Daten\n",
    "mean_in = torch.mean(x_input_train,0) # Mittelwert\n",
    "std_in = torch.std(x_input_train,0) # Standardabweichung\n",
    "mean_out = torch.mean(y_output_train,0)\n",
    "std_out = torch.std(y_output_train,0)\n",
    "\n",
    "x_input_train_norm = (x_input_train - mean_in) / std_in\n",
    "y_output_train_norm = (y_output_train - mean_out) / std_out\n",
    "\n",
    "x_input_test_norm = (x_input_test - mean_in) / std_in\n",
    "y_output_test_norm = (y_output_test - mean_out) / std_out\n",
    "\n",
    "# print(x_input_train_norm)\n",
    "# print(torch.mean(x_input_train_norm[:,0]))\n",
    "\n",
    "# Tensoren zu einem großen Set gruppieren\n",
    "train_dataset = TensorDataset(x_input_train_norm, y_output_train_norm)\n",
    "test_dataset = TensorDataset(x_input_test_norm, y_output_test_norm)\n",
    "    \n",
    "# # Split in Trainings und Test Set\n",
    "# train_dataset, test_dataset = random_split(dataset, \n",
    "#                                            [int(0.8*len(dataset)), int(0.2*len(dataset))], # splitting 80/20\n",
    "#                                            generator = torch.Generator().manual_seed(42)) # Festlegung seed zur Reproduktivität\n",
    "\n",
    "# Erzeugen der DataLoader zur Arbeit mit Daten\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True) # shuffle batches zur Reduzierung von overfitting\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e9841",
   "metadata": {},
   "source": [
    "#### Generierung Netzwerk, Festlegung von loss Funktion und Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ab5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung aNN\n",
    "net = NeuralNetwork(5, 200, 2)\n",
    "\n",
    "# Loss Funktion; gibt Fehler an\n",
    "loss_fn_MSE = nn.MSELoss()\n",
    "loss_fn_L1 = nn.L1Loss()\n",
    "\n",
    "#Definition custom loss Funktion, MRE\n",
    "def MRELoss(outputs, targets):\n",
    "    \n",
    "    loss = torch.mean(abs((outputs - targets) / targets))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "#Optimizer\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "#scheduler = StepLR(optimizer, step_size = 30, gamma = 0.1)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[30, 70, 100], gamma = 0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 10, threshold = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ccc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-6\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b61b7",
   "metadata": {},
   "source": [
    "#### Funktion zur Bestimmung der Genauigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4480b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, net):\n",
    "    \n",
    "    loss = 0\n",
    "    MRE = 0\n",
    "    MAE = 0\n",
    "    train_correct = 0\n",
    "    train_total = len(loader.dataset)\n",
    "    num_batches = len(loader) \n",
    "    #train_total = 0\n",
    "    \n",
    "    net.eval() # Put network in evaluation mode\n",
    "    \n",
    "    if loader == train_dataloader:\n",
    "        dataset = \"Train\"\n",
    "    else:\n",
    "        dataset = \"Test\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            pred = net(X)\n",
    "           \n",
    "            #loss += MRELoss(pred, y).item()\n",
    "            loss += loss_fn_MSE(pred, y) # Calculate the loss\n",
    "            MRE += MRELoss(pred * std_out + mean_out, y * std_out + mean_out)\n",
    "            MAE += loss_fn_L1(pred * std_out + mean_out, y * std_out + mean_out)\n",
    "            \n",
    "            # Record the correct predictions for training data\n",
    "            #_, predictions = torch.max(pred.data, 1)\n",
    "            for i in range(len(pred)):\n",
    "                if ((pred[i,0] * std_out[0] + mean_out[0]) - (y[i,0] * std_out[0] + mean_out[0]) and (pred[i,1] * std_out[1] + mean_out[1]) - (y[i,1] * std_out[1] + mean_out[1])) <= 0.01:\n",
    "                    train_correct += 1\n",
    "            #train_correct += (abs(pred.argmax(1) - y) <= 0.01).sum().item()\n",
    "            #train_correct += (abs(predictions - y.data) <= 0.01).sum()\n",
    "            #train_total += predictions.size(0)\n",
    "            \n",
    "        # Genauigkeit berechnen\n",
    "        acc = float(train_correct) / float(train_total) * 100\n",
    "        acc = round(acc, 2)\n",
    "        \n",
    "        loss /= num_batches\n",
    "        MRE /= num_batches\n",
    "        MAE /= num_batches\n",
    "\n",
    "        print(f\"{dataset} Error: \\n Accuracy: {acc}%, Avg loss: {loss:>8f}, MRE: {MRE:>8f}, MAE: {MAE:>8f} \\n\")\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    return acc, loss, MRE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd049ed",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771789d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Iteration 1/12, Loss: 1.0242\n",
      "Epoch 1/200, Iteration 2/12, Loss: 0.6264\n",
      "Epoch 1/200, Iteration 3/12, Loss: 0.3215\n",
      "Epoch 1/200, Iteration 4/12, Loss: 0.1031\n",
      "Epoch 1/200, Iteration 5/12, Loss: 0.0684\n",
      "Epoch 1/200, Iteration 6/12, Loss: 0.1807\n",
      "Epoch 1/200, Iteration 7/12, Loss: 0.1746\n",
      "Epoch 1/200, Iteration 8/12, Loss: 0.3021\n",
      "Epoch 1/200, Iteration 9/12, Loss: 0.3142\n",
      "Epoch 1/200, Iteration 10/12, Loss: 0.2181\n",
      "Epoch 1/200, Iteration 11/12, Loss: 0.1587\n",
      "Epoch 1/200, Iteration 12/12, Loss: 0.0787\n",
      "Epoch 1/200, Iteration 13/12, Loss: 0.0908\n",
      "Train Error: \n",
      " Accuracy: 84.12%, Avg loss: 0.073451, MRE: 0.149371, MAE: 0.024055 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.080414, MRE: 0.137976, MAE: 0.024650 \n",
      "\n",
      "Epoch 2/200, Iteration 1/12, Loss: 0.0921\n",
      "Epoch 2/200, Iteration 2/12, Loss: 0.1003\n",
      "Epoch 2/200, Iteration 3/12, Loss: 0.0475\n",
      "Epoch 2/200, Iteration 4/12, Loss: 0.0576\n",
      "Epoch 2/200, Iteration 5/12, Loss: 0.0766\n",
      "Epoch 2/200, Iteration 6/12, Loss: 0.0801\n",
      "Epoch 2/200, Iteration 7/12, Loss: 0.1201\n",
      "Epoch 2/200, Iteration 8/12, Loss: 0.0882\n",
      "Epoch 2/200, Iteration 9/12, Loss: 0.0510\n",
      "Epoch 2/200, Iteration 10/12, Loss: 0.0464\n",
      "Epoch 2/200, Iteration 11/12, Loss: 0.0490\n",
      "Epoch 2/200, Iteration 12/12, Loss: 0.0500\n",
      "Epoch 2/200, Iteration 13/12, Loss: 0.0411\n",
      "Train Error: \n",
      " Accuracy: 68.25%, Avg loss: 0.034640, MRE: 0.105300, MAE: 0.016035 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.037362, MRE: 0.090776, MAE: 0.017405 \n",
      "\n",
      "Epoch 3/200, Iteration 1/12, Loss: 0.0284\n",
      "Epoch 3/200, Iteration 2/12, Loss: 0.0210\n",
      "Epoch 3/200, Iteration 3/12, Loss: 0.0503\n",
      "Epoch 3/200, Iteration 4/12, Loss: 0.0293\n",
      "Epoch 3/200, Iteration 5/12, Loss: 0.0487\n",
      "Epoch 3/200, Iteration 6/12, Loss: 0.0511\n",
      "Epoch 3/200, Iteration 7/12, Loss: 0.0425\n",
      "Epoch 3/200, Iteration 8/12, Loss: 0.0590\n",
      "Epoch 3/200, Iteration 9/12, Loss: 0.0346\n",
      "Epoch 3/200, Iteration 10/12, Loss: 0.0361\n",
      "Epoch 3/200, Iteration 11/12, Loss: 0.0193\n",
      "Epoch 3/200, Iteration 12/12, Loss: 0.0277\n",
      "Epoch 3/200, Iteration 13/12, Loss: 0.0225\n",
      "Train Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.028095, MRE: 0.090970, MAE: 0.012810 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.019198, MRE: 0.069030, MAE: 0.011340 \n",
      "\n",
      "Epoch 4/200, Iteration 1/12, Loss: 0.0720\n",
      "Epoch 4/200, Iteration 2/12, Loss: 0.0205\n",
      "Epoch 4/200, Iteration 3/12, Loss: 0.0166\n",
      "Epoch 4/200, Iteration 4/12, Loss: 0.0342\n",
      "Epoch 4/200, Iteration 5/12, Loss: 0.0188\n",
      "Epoch 4/200, Iteration 6/12, Loss: 0.0210\n",
      "Epoch 4/200, Iteration 7/12, Loss: 0.0151\n",
      "Epoch 4/200, Iteration 8/12, Loss: 0.0343\n",
      "Epoch 4/200, Iteration 9/12, Loss: 0.0255\n",
      "Epoch 4/200, Iteration 10/12, Loss: 0.0101\n",
      "Epoch 4/200, Iteration 11/12, Loss: 0.0155\n",
      "Epoch 4/200, Iteration 12/12, Loss: 0.0282\n",
      "Epoch 4/200, Iteration 13/12, Loss: 0.0463\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.023371, MRE: 0.094003, MAE: 0.011212 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.019084, MRE: 0.069888, MAE: 0.011304 \n",
      "\n",
      "Epoch 5/200, Iteration 1/12, Loss: 0.0207\n",
      "Epoch 5/200, Iteration 2/12, Loss: 0.0120\n",
      "Epoch 5/200, Iteration 3/12, Loss: 0.0341\n",
      "Epoch 5/200, Iteration 4/12, Loss: 0.0193\n",
      "Epoch 5/200, Iteration 5/12, Loss: 0.0185\n",
      "Epoch 5/200, Iteration 6/12, Loss: 0.0113\n",
      "Epoch 5/200, Iteration 7/12, Loss: 0.0162\n",
      "Epoch 5/200, Iteration 8/12, Loss: 0.0194\n",
      "Epoch 5/200, Iteration 9/12, Loss: 0.0383\n",
      "Epoch 5/200, Iteration 10/12, Loss: 0.0142\n",
      "Epoch 5/200, Iteration 11/12, Loss: 0.0171\n",
      "Epoch 5/200, Iteration 12/12, Loss: 0.0603\n",
      "Epoch 5/200, Iteration 13/12, Loss: 0.0324\n",
      "Train Error: \n",
      " Accuracy: 86.12%, Avg loss: 0.025603, MRE: 0.083846, MAE: 0.011892 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.029824, MRE: 0.071976, MAE: 0.013281 \n",
      "\n",
      "Epoch 6/200, Iteration 1/12, Loss: 0.0557\n",
      "Epoch 6/200, Iteration 2/12, Loss: 0.0225\n",
      "Epoch 6/200, Iteration 3/12, Loss: 0.0242\n",
      "Epoch 6/200, Iteration 4/12, Loss: 0.0214\n",
      "Epoch 6/200, Iteration 5/12, Loss: 0.0201\n",
      "Epoch 6/200, Iteration 6/12, Loss: 0.0196\n",
      "Epoch 6/200, Iteration 7/12, Loss: 0.0305\n",
      "Epoch 6/200, Iteration 8/12, Loss: 0.0264\n",
      "Epoch 6/200, Iteration 9/12, Loss: 0.0280\n",
      "Epoch 6/200, Iteration 10/12, Loss: 0.0109\n",
      "Epoch 6/200, Iteration 11/12, Loss: 0.0086\n",
      "Epoch 6/200, Iteration 12/12, Loss: 0.0154\n",
      "Epoch 6/200, Iteration 13/12, Loss: 0.0101\n",
      "Train Error: \n",
      " Accuracy: 88.88%, Avg loss: 0.020322, MRE: 0.096736, MAE: 0.010165 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.016637, MRE: 0.069882, MAE: 0.010364 \n",
      "\n",
      "Epoch 7/200, Iteration 1/12, Loss: 0.0156\n",
      "Epoch 7/200, Iteration 2/12, Loss: 0.0285\n",
      "Epoch 7/200, Iteration 3/12, Loss: 0.0134\n",
      "Epoch 7/200, Iteration 4/12, Loss: 0.0320\n",
      "Epoch 7/200, Iteration 5/12, Loss: 0.0192\n",
      "Epoch 7/200, Iteration 6/12, Loss: 0.0326\n",
      "Epoch 7/200, Iteration 7/12, Loss: 0.0188\n",
      "Epoch 7/200, Iteration 8/12, Loss: 0.0134\n",
      "Epoch 7/200, Iteration 9/12, Loss: 0.0197\n",
      "Epoch 7/200, Iteration 10/12, Loss: 0.0197\n",
      "Epoch 7/200, Iteration 11/12, Loss: 0.0134\n",
      "Epoch 7/200, Iteration 12/12, Loss: 0.0187\n",
      "Epoch 7/200, Iteration 13/12, Loss: 0.0108\n",
      "Train Error: \n",
      " Accuracy: 79.25%, Avg loss: 0.019395, MRE: 0.073946, MAE: 0.009431 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.011838, MRE: 0.055412, MAE: 0.008826 \n",
      "\n",
      "Epoch 8/200, Iteration 1/12, Loss: 0.0092\n",
      "Epoch 8/200, Iteration 2/12, Loss: 0.0147\n",
      "Epoch 8/200, Iteration 3/12, Loss: 0.0139\n",
      "Epoch 8/200, Iteration 4/12, Loss: 0.0075\n",
      "Epoch 8/200, Iteration 5/12, Loss: 0.0141\n",
      "Epoch 8/200, Iteration 6/12, Loss: 0.0247\n",
      "Epoch 8/200, Iteration 7/12, Loss: 0.0330\n",
      "Epoch 8/200, Iteration 8/12, Loss: 0.0158\n",
      "Epoch 8/200, Iteration 9/12, Loss: 0.0211\n",
      "Epoch 8/200, Iteration 10/12, Loss: 0.0209\n",
      "Epoch 8/200, Iteration 11/12, Loss: 0.0332\n",
      "Epoch 8/200, Iteration 12/12, Loss: 0.0241\n",
      "Epoch 8/200, Iteration 13/12, Loss: 0.0288\n",
      "Train Error: \n",
      " Accuracy: 79.25%, Avg loss: 0.018327, MRE: 0.063795, MAE: 0.009253 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.011687, MRE: 0.049986, MAE: 0.008686 \n",
      "\n",
      "Epoch 9/200, Iteration 1/12, Loss: 0.0133\n",
      "Epoch 9/200, Iteration 2/12, Loss: 0.0422\n",
      "Epoch 9/200, Iteration 3/12, Loss: 0.0141\n",
      "Epoch 9/200, Iteration 4/12, Loss: 0.0078\n",
      "Epoch 9/200, Iteration 5/12, Loss: 0.0219\n",
      "Epoch 9/200, Iteration 6/12, Loss: 0.0198\n",
      "Epoch 9/200, Iteration 7/12, Loss: 0.0168\n",
      "Epoch 9/200, Iteration 8/12, Loss: 0.0137\n",
      "Epoch 9/200, Iteration 9/12, Loss: 0.0221\n",
      "Epoch 9/200, Iteration 10/12, Loss: 0.0237\n",
      "Epoch 9/200, Iteration 11/12, Loss: 0.0166\n",
      "Epoch 9/200, Iteration 12/12, Loss: 0.0136\n",
      "Epoch 9/200, Iteration 13/12, Loss: 0.0169\n",
      "Train Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.015918, MRE: 0.057880, MAE: 0.008546 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.014030, MRE: 0.048611, MAE: 0.008931 \n",
      "\n",
      "Epoch 10/200, Iteration 1/12, Loss: 0.0081\n",
      "Epoch 10/200, Iteration 2/12, Loss: 0.0234\n",
      "Epoch 10/200, Iteration 3/12, Loss: 0.0148\n",
      "Epoch 10/200, Iteration 4/12, Loss: 0.0225\n",
      "Epoch 10/200, Iteration 5/12, Loss: 0.0228\n",
      "Epoch 10/200, Iteration 6/12, Loss: 0.0095\n",
      "Epoch 10/200, Iteration 7/12, Loss: 0.0172\n",
      "Epoch 10/200, Iteration 8/12, Loss: 0.0072\n",
      "Epoch 10/200, Iteration 9/12, Loss: 0.0182\n",
      "Epoch 10/200, Iteration 10/12, Loss: 0.0156\n",
      "Epoch 10/200, Iteration 11/12, Loss: 0.0232\n",
      "Epoch 10/200, Iteration 12/12, Loss: 0.0246\n",
      "Epoch 10/200, Iteration 13/12, Loss: 0.0092\n",
      "Train Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.013767, MRE: 0.055264, MAE: 0.008554 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.011752, MRE: 0.044540, MAE: 0.009072 \n",
      "\n",
      "Epoch 11/200, Iteration 1/12, Loss: 0.0101\n",
      "Epoch 11/200, Iteration 2/12, Loss: 0.0208\n",
      "Epoch 11/200, Iteration 3/12, Loss: 0.0252\n",
      "Epoch 11/200, Iteration 4/12, Loss: 0.0139\n",
      "Epoch 11/200, Iteration 5/12, Loss: 0.0194\n",
      "Epoch 11/200, Iteration 6/12, Loss: 0.0166\n",
      "Epoch 11/200, Iteration 7/12, Loss: 0.0172\n",
      "Epoch 11/200, Iteration 8/12, Loss: 0.0098\n",
      "Epoch 11/200, Iteration 9/12, Loss: 0.0115\n",
      "Epoch 11/200, Iteration 10/12, Loss: 0.0057\n",
      "Epoch 11/200, Iteration 11/12, Loss: 0.0240\n",
      "Epoch 11/200, Iteration 12/12, Loss: 0.0079\n",
      "Epoch 11/200, Iteration 13/12, Loss: 0.0052\n",
      "Train Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.012187, MRE: 0.053845, MAE: 0.008279 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.010384, MRE: 0.044326, MAE: 0.008641 \n",
      "\n",
      "Epoch 12/200, Iteration 1/12, Loss: 0.0054\n",
      "Epoch 12/200, Iteration 2/12, Loss: 0.0091\n",
      "Epoch 12/200, Iteration 3/12, Loss: 0.0187\n",
      "Epoch 12/200, Iteration 4/12, Loss: 0.0092\n",
      "Epoch 12/200, Iteration 5/12, Loss: 0.0137\n",
      "Epoch 12/200, Iteration 6/12, Loss: 0.0049\n",
      "Epoch 12/200, Iteration 7/12, Loss: 0.0297\n",
      "Epoch 12/200, Iteration 8/12, Loss: 0.0114\n",
      "Epoch 12/200, Iteration 9/12, Loss: 0.0122\n",
      "Epoch 12/200, Iteration 10/12, Loss: 0.0134\n",
      "Epoch 12/200, Iteration 11/12, Loss: 0.0104\n",
      "Epoch 12/200, Iteration 12/12, Loss: 0.0186\n",
      "Epoch 12/200, Iteration 13/12, Loss: 0.0083\n",
      "Train Error: \n",
      " Accuracy: 95.88%, Avg loss: 0.010457, MRE: 0.049008, MAE: 0.007346 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.009110, MRE: 0.038877, MAE: 0.007239 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Iteration 1/12, Loss: 0.0129\n",
      "Epoch 13/200, Iteration 2/12, Loss: 0.0188\n",
      "Epoch 13/200, Iteration 3/12, Loss: 0.0060\n",
      "Epoch 13/200, Iteration 4/12, Loss: 0.0077\n",
      "Epoch 13/200, Iteration 5/12, Loss: 0.0086\n",
      "Epoch 13/200, Iteration 6/12, Loss: 0.0207\n",
      "Epoch 13/200, Iteration 7/12, Loss: 0.0277\n",
      "Epoch 13/200, Iteration 8/12, Loss: 0.0092\n",
      "Epoch 13/200, Iteration 9/12, Loss: 0.0065\n",
      "Epoch 13/200, Iteration 10/12, Loss: 0.0120\n",
      "Epoch 13/200, Iteration 11/12, Loss: 0.0075\n",
      "Epoch 13/200, Iteration 12/12, Loss: 0.0093\n",
      "Epoch 13/200, Iteration 13/12, Loss: 0.0135\n",
      "Train Error: \n",
      " Accuracy: 83.88%, Avg loss: 0.012080, MRE: 0.048344, MAE: 0.008349 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.008736, MRE: 0.040536, MAE: 0.007931 \n",
      "\n",
      "Epoch 14/200, Iteration 1/12, Loss: 0.0080\n",
      "Epoch 14/200, Iteration 2/12, Loss: 0.0138\n",
      "Epoch 14/200, Iteration 3/12, Loss: 0.0147\n",
      "Epoch 14/200, Iteration 4/12, Loss: 0.0076\n",
      "Epoch 14/200, Iteration 5/12, Loss: 0.0097\n",
      "Epoch 14/200, Iteration 6/12, Loss: 0.0121\n",
      "Epoch 14/200, Iteration 7/12, Loss: 0.0120\n",
      "Epoch 14/200, Iteration 8/12, Loss: 0.0241\n",
      "Epoch 14/200, Iteration 9/12, Loss: 0.0187\n",
      "Epoch 14/200, Iteration 10/12, Loss: 0.0053\n",
      "Epoch 14/200, Iteration 11/12, Loss: 0.0122\n",
      "Epoch 14/200, Iteration 12/12, Loss: 0.0355\n",
      "Epoch 14/200, Iteration 13/12, Loss: 0.0054\n",
      "Train Error: \n",
      " Accuracy: 89.38%, Avg loss: 0.010267, MRE: 0.040578, MAE: 0.007066 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.006254, MRE: 0.037277, MAE: 0.006478 \n",
      "\n",
      "Epoch 15/200, Iteration 1/12, Loss: 0.0102\n",
      "Epoch 15/200, Iteration 2/12, Loss: 0.0070\n",
      "Epoch 15/200, Iteration 3/12, Loss: 0.0098\n",
      "Epoch 15/200, Iteration 4/12, Loss: 0.0201\n",
      "Epoch 15/200, Iteration 5/12, Loss: 0.0087\n",
      "Epoch 15/200, Iteration 6/12, Loss: 0.0148\n",
      "Epoch 15/200, Iteration 7/12, Loss: 0.0173\n",
      "Epoch 15/200, Iteration 8/12, Loss: 0.0071\n",
      "Epoch 15/200, Iteration 9/12, Loss: 0.0082\n",
      "Epoch 15/200, Iteration 10/12, Loss: 0.0087\n",
      "Epoch 15/200, Iteration 11/12, Loss: 0.0086\n",
      "Epoch 15/200, Iteration 12/12, Loss: 0.0040\n",
      "Epoch 15/200, Iteration 13/12, Loss: 0.0076\n",
      "Train Error: \n",
      " Accuracy: 89.25%, Avg loss: 0.009348, MRE: 0.048474, MAE: 0.006988 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.006651, MRE: 0.039106, MAE: 0.006700 \n",
      "\n",
      "Epoch 16/200, Iteration 1/12, Loss: 0.0070\n",
      "Epoch 16/200, Iteration 2/12, Loss: 0.0044\n",
      "Epoch 16/200, Iteration 3/12, Loss: 0.0070\n",
      "Epoch 16/200, Iteration 4/12, Loss: 0.0059\n",
      "Epoch 16/200, Iteration 5/12, Loss: 0.0118\n",
      "Epoch 16/200, Iteration 6/12, Loss: 0.0122\n",
      "Epoch 16/200, Iteration 7/12, Loss: 0.0044\n",
      "Epoch 16/200, Iteration 8/12, Loss: 0.0123\n",
      "Epoch 16/200, Iteration 9/12, Loss: 0.0103\n",
      "Epoch 16/200, Iteration 10/12, Loss: 0.0201\n",
      "Epoch 16/200, Iteration 11/12, Loss: 0.0053\n",
      "Epoch 16/200, Iteration 12/12, Loss: 0.0074\n",
      "Epoch 16/200, Iteration 13/12, Loss: 0.0205\n",
      "Train Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.007860, MRE: 0.042114, MAE: 0.006242 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.005998, MRE: 0.033972, MAE: 0.006027 \n",
      "\n",
      "Epoch 17/200, Iteration 1/12, Loss: 0.0061\n",
      "Epoch 17/200, Iteration 2/12, Loss: 0.0045\n",
      "Epoch 17/200, Iteration 3/12, Loss: 0.0081\n",
      "Epoch 17/200, Iteration 4/12, Loss: 0.0130\n",
      "Epoch 17/200, Iteration 5/12, Loss: 0.0067\n",
      "Epoch 17/200, Iteration 6/12, Loss: 0.0067\n",
      "Epoch 17/200, Iteration 7/12, Loss: 0.0101\n",
      "Epoch 17/200, Iteration 8/12, Loss: 0.0102\n",
      "Epoch 17/200, Iteration 9/12, Loss: 0.0133\n",
      "Epoch 17/200, Iteration 10/12, Loss: 0.0059\n",
      "Epoch 17/200, Iteration 11/12, Loss: 0.0075\n",
      "Epoch 17/200, Iteration 12/12, Loss: 0.0149\n",
      "Epoch 17/200, Iteration 13/12, Loss: 0.0120\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.008023, MRE: 0.038445, MAE: 0.006366 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.005803, MRE: 0.033671, MAE: 0.005977 \n",
      "\n",
      "Epoch 18/200, Iteration 1/12, Loss: 0.0126\n",
      "Epoch 18/200, Iteration 2/12, Loss: 0.0076\n",
      "Epoch 18/200, Iteration 3/12, Loss: 0.0068\n",
      "Epoch 18/200, Iteration 4/12, Loss: 0.0056\n",
      "Epoch 18/200, Iteration 5/12, Loss: 0.0093\n",
      "Epoch 18/200, Iteration 6/12, Loss: 0.0041\n",
      "Epoch 18/200, Iteration 7/12, Loss: 0.0108\n",
      "Epoch 18/200, Iteration 8/12, Loss: 0.0049\n",
      "Epoch 18/200, Iteration 9/12, Loss: 0.0114\n",
      "Epoch 18/200, Iteration 10/12, Loss: 0.0059\n",
      "Epoch 18/200, Iteration 11/12, Loss: 0.0057\n",
      "Epoch 18/200, Iteration 12/12, Loss: 0.0132\n",
      "Epoch 18/200, Iteration 13/12, Loss: 0.0105\n",
      "Train Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.006933, MRE: 0.036381, MAE: 0.005765 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.005519, MRE: 0.030685, MAE: 0.005410 \n",
      "\n",
      "Epoch 19/200, Iteration 1/12, Loss: 0.0078\n",
      "Epoch 19/200, Iteration 2/12, Loss: 0.0042\n",
      "Epoch 19/200, Iteration 3/12, Loss: 0.0107\n",
      "Epoch 19/200, Iteration 4/12, Loss: 0.0041\n",
      "Epoch 19/200, Iteration 5/12, Loss: 0.0258\n",
      "Epoch 19/200, Iteration 6/12, Loss: 0.0070\n",
      "Epoch 19/200, Iteration 7/12, Loss: 0.0111\n",
      "Epoch 19/200, Iteration 8/12, Loss: 0.0071\n",
      "Epoch 19/200, Iteration 9/12, Loss: 0.0040\n",
      "Epoch 19/200, Iteration 10/12, Loss: 0.0149\n",
      "Epoch 19/200, Iteration 11/12, Loss: 0.0050\n",
      "Epoch 19/200, Iteration 12/12, Loss: 0.0097\n",
      "Epoch 19/200, Iteration 13/12, Loss: 0.0070\n",
      "Train Error: \n",
      " Accuracy: 81.62%, Avg loss: 0.009092, MRE: 0.049613, MAE: 0.006748 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.006318, MRE: 0.039195, MAE: 0.006449 \n",
      "\n",
      "Epoch 20/200, Iteration 1/12, Loss: 0.0071\n",
      "Epoch 20/200, Iteration 2/12, Loss: 0.0059\n",
      "Epoch 20/200, Iteration 3/12, Loss: 0.0074\n",
      "Epoch 20/200, Iteration 4/12, Loss: 0.0040\n",
      "Epoch 20/200, Iteration 5/12, Loss: 0.0169\n",
      "Epoch 20/200, Iteration 6/12, Loss: 0.0050\n",
      "Epoch 20/200, Iteration 7/12, Loss: 0.0086\n",
      "Epoch 20/200, Iteration 8/12, Loss: 0.0044\n",
      "Epoch 20/200, Iteration 9/12, Loss: 0.0068\n",
      "Epoch 20/200, Iteration 10/12, Loss: 0.0064\n",
      "Epoch 20/200, Iteration 11/12, Loss: 0.0059\n",
      "Epoch 20/200, Iteration 12/12, Loss: 0.0125\n",
      "Epoch 20/200, Iteration 13/12, Loss: 0.0076\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200 #Iterationen über Datenset\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_MRE = []\n",
    "test_MRE = []\n",
    "train_MAE = []\n",
    "test_MAE = []\n",
    "\n",
    "#Runtime measurement\n",
    "train_time_start = time.process_time()\n",
    "#Optimierungsloop\n",
    "for epoch in range(num_epochs):\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "        \n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        \n",
    "#         print(X.shape)\n",
    "#         print(X.dtype)\n",
    "        \n",
    "        net.train() #Trainingmodus\n",
    "        \n",
    "        # forward\n",
    "        pred = net(X)  # Do the forward pass\n",
    "        loss = loss_fn_MSE(pred, y) # Calculate the loss\n",
    "        #loss = MRELoss(pred, y)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad() # Clear off the gradients from any past operation\n",
    "        loss.backward()       # Calculate the gradients with help of back propagation, updating weights and biases\n",
    "        \n",
    "        # adam step gradient descent\n",
    "        optimizer.step()      # Ask the optimizer to adjust the parameters based on the gradients  \n",
    "\n",
    "        print ('Epoch %d/%d, Iteration %d/%d, Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, batch+1, len(train_dataset)//batch_size, loss.item()))\n",
    "        \n",
    "    \n",
    "    #scheduler.step() # Reduzieren Learning Rate (falls step size erreicht)\n",
    "    net.eval() # Put the network into evaluation mode\n",
    "    \n",
    "    # Book keeping    \n",
    "    # What was our train accuracy?\n",
    "    tr_acc, tr_loss, tr_MRE, tr_MAE = check_accuracy(train_dataloader, net)\n",
    "    \n",
    "    #Record loss and accuracy\n",
    "    train_accuracy.append(tr_acc)\n",
    "    train_loss.append(tr_loss)\n",
    "    train_MRE.append(tr_MRE)\n",
    "    train_MAE.append(tr_MAE)\n",
    "    \n",
    "    scheduler.step(tr_loss) # LR scheduler step für reduceonPlateau\n",
    "    \n",
    "    # How did we do on the test set (the unseen set)\n",
    "    # Record the correct predictions for test data\n",
    "    t_acc, t_loss, t_MRE, t_MAE = check_accuracy(test_dataloader, net)\n",
    "    test_accuracy.append(t_acc)\n",
    "    test_loss.append(t_loss)\n",
    "    test_MRE.append(t_MRE)\n",
    "    test_MAE.append(t_MAE)\n",
    "\n",
    "train_time = time.process_time() - train_time_start\n",
    "print('Training time:',train_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9fb4a",
   "metadata": {},
   "source": [
    "#### Plots loss vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#fig.figsize=(12, 8)\n",
    "ax.semilogy(train_loss, label='train loss')\n",
    "ax.semilogy(test_loss, label='test loss')\n",
    "plt.title(\"Train and Test Loss\")\n",
    "ax.set(xlabel = '$Epochs$ / 1', ylabel = 'Loss / 1') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f9848",
   "metadata": {},
   "source": [
    "#### Parity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_H2_real_norm = []\n",
    "x_H2_pred_norm = []\n",
    "x_NH3_real_norm = []\n",
    "x_NH3_pred_norm = []\n",
    "for (X,y) in train_dataloader:\n",
    "    x_H2_real_norm = np.append(x_H2_real_norm, y[:,0].numpy())\n",
    "    x_NH3_real_norm = np.append(x_NH3_real_norm, y[:,1].numpy())\n",
    "    help_x_H2,help_x_NH3 = (net(X).detach().numpy()).T\n",
    "    x_H2_pred_norm = np.append(x_H2_pred_norm, help_x_H2)\n",
    "    x_NH3_pred_norm = np.append(x_NH3_pred_norm, help_x_NH3)\n",
    "\n",
    "x_H2_real_test_norm = []\n",
    "x_H2_pred_test_norm = []\n",
    "x_NH3_real_test_norm = []\n",
    "x_NH3_pred_test_norm = []\n",
    "for (X,y) in test_dataloader:\n",
    "    x_H2_real_test_norm = np.append(x_H2_real_test_norm, y[:,0].numpy())\n",
    "    x_NH3_real_test_norm = np.append(x_NH3_real_test_norm, y[:,1].numpy())\n",
    "    help_x_H2,help_x_NH3 = (net(X).detach().numpy()).T\n",
    "    x_H2_pred_test_norm = np.append(x_H2_pred_test_norm, help_x_H2)\n",
    "    x_NH3_pred_test_norm = np.append(x_NH3_pred_test_norm, help_x_NH3)\n",
    "\n",
    "x_H2_real = x_H2_real_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_pred = x_H2_pred_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_real = x_NH3_real_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_pred = x_NH3_pred_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "x_H2_real_test = x_H2_real_test_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_pred_test = x_H2_pred_test_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_real_test = x_NH3_real_test_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_pred_test = x_NH3_pred_test_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "print('Training Dataset: R^2(H2) =', r2(x_H2_real,x_H2_pred), ', R^2(NH3) =', r2(x_NH3_real,x_NH3_pred))\n",
    "print('Test Dataset: R^2(H2) =', r2(x_H2_real_test,x_H2_pred_test), ', R^2(NH3) =', r2(x_NH3_real_test,x_NH3_pred_test))\n",
    "print('Max Error Training: |x_H2 - x_H2,pred| =', max_error(x_H2_real, x_H2_pred), ', |x_NH3 - x_NH3,pred| =', max_error(x_NH3_real, x_NH3_pred))\n",
    "print('Max Error Test: |x_H2 - x_H2,pred| =', max_error(x_H2_real_test, x_H2_pred_test), ', |x_NH3 - x_NH3,pred| =', max_error(x_NH3_real_test, x_NH3_pred_test))\n",
    "\n",
    "# find the boundaries of X and Y values\n",
    "bounds = (0,1)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize =(10,10))\n",
    "\n",
    "# # Reset the limits\n",
    "# ax[0] = plt.gca()\n",
    "ax[0].set_xlim(bounds)\n",
    "ax[0].set_ylim(bounds)\n",
    "# Ensure the aspect ratio is square\n",
    "ax[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "ax[0].plot(x_H2_real, x_H2_pred, '.', color ='rebeccapurple', label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0].plot(x_NH3_real, x_NH3_pred, '.', color ='cornflowerblue', label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0].plot([0, 1], [0, 1], \"r-\",lw=2 ,transform=ax[0].transAxes)\n",
    "ax[0].plot([bounds[0],bounds[1]], [bounds[0] * 1.1, bounds[1] * 1.1], \"k--\") # Error line\n",
    "ax[0].plot([bounds[0],bounds[1]], [bounds[0] * 0.9, bounds[1] * 0.9], \"k--\") # Error line\n",
    "ax[0].text(0.7, 0.9, '+10%')\n",
    "ax[0].text(0.8, 0.65, '-10%')\n",
    "ax[0].set(xlabel = '$x$ / 1', ylabel = '$x\\mathregular{_{pred}}$ / 1')\n",
    "ax[0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0].set_title('Train Data')\n",
    "ax[0].legend()\n",
    "#ax[0].legend(['$\\\\mathregular{R^2}$ = ', r2(xi_real,xi_pred)], markerscale=0)\n",
    "\n",
    "# Reset the limits\n",
    "#ax[1] = plt.gca()\n",
    "ax[1].set_xlim(bounds)\n",
    "ax[1].set_ylim(bounds)\n",
    "# Ensure the aspect ratio is square\n",
    "ax[1].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "ax[1].plot(x_H2_real_test, x_H2_pred_test, '.', color ='rebeccapurple', label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[1].plot(x_NH3_real_test, x_NH3_pred_test, '.', color ='cornflowerblue', label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[1].plot([0, 1], [0, 1], \"r-\",lw=2 ,transform=ax[1].transAxes)\n",
    "ax[1].plot([bounds[0],bounds[1]], [bounds[0] * 1.1, bounds[1] * 1.1], \"k--\") # Error line\n",
    "ax[1].plot([bounds[0],bounds[1]], [bounds[0] * 0.9, bounds[1] * 0.9], \"k--\") # Error line\n",
    "ax[1].text(0.7, 0.9, '+10%')\n",
    "ax[1].text(0.8, 0.65, '-10%')\n",
    "ax[1].set(xlabel = '$x$ / 1', ylabel = '$x\\mathregular{_{pred}}$ / 1')\n",
    "ax[1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1].set_title('Test Data')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "#fig.suptitle(\"Parity Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.semilogy(x_H2_real, abs((x_H2_pred - x_H2_real) / x_H2_real), '.', label = 'MRE')\n",
    "ax.semilogy(x_H2_real, abs(x_H2_real-x_H2_pred), '.', label = 'MAE')\n",
    "ax.set(xlabel = '$x \\mathregular{_{H_2}}$ / 1', ylabel = 'Mistake')\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4390bab",
   "metadata": {},
   "source": [
    "#### Plot Fehler vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2)\n",
    "#fig.figsize=(12, 8)\n",
    "ax[0].semilogy(train_MRE, label='train MRE')\n",
    "ax[0].semilogy(test_MRE, label='test MRE')\n",
    "ax[0].set_title(\"Mean Relative Error\")\n",
    "ax[0].set(xlabel = '$Epochs$ / 1', ylabel = '$MRE$ / 1') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax[0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].semilogy(train_MAE, label='train MAE')\n",
    "ax[1].semilogy(test_MAE, label='test MAE')\n",
    "ax[1].set_title(\"Mean Absolute Error\")\n",
    "ax[1].set(xlabel = '$Epochs$ / 1', ylabel = '$MAE$ / mol') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax[1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbf7c2",
   "metadata": {},
   "source": [
    "#### Plot Loss vs Variable Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistake_H2 = []\n",
    "# mistake_NH3 = []\n",
    "x_H2_pred_norm = []\n",
    "x_NH3_pred_norm = []\n",
    "x_H2_real_norm = []\n",
    "x_NH3_real_norm = []\n",
    "param_T_norm = []\n",
    "param_p_norm = []\n",
    "param_x_H2_0_norm = []\n",
    "param_x_N2_0_norm = []\n",
    "param_x_NH3_0_norm = []\n",
    "for X,y in train_dataloader:\n",
    "#     help_mistake_H2, help_mistake_NH3 = (abs(y - net(X).detach().numpy())).T\n",
    "#     mistake_H2 = np.append(mistake_H2, help_mistake_H2)\n",
    "#     mistake_NH3 = np.append(mistake_NH3, help_mistake_NH3\n",
    "    help_pred = net(X).detach().numpy()\n",
    "    x_H2_pred_norm = np.append(x_H2_pred_norm, help_pred[:,0])\n",
    "    x_NH3_pred_norm = np.append(x_NH3_pred_norm, help_pred[:,1])\n",
    "    help_real = y.detach().numpy()\n",
    "    x_H2_real_norm = np.append(x_H2_real_norm, help_real[:,0])\n",
    "    x_NH3_real_norm = np.append(x_NH3_real_norm, help_real[:,1])\n",
    "    param_T_norm = np.append(param_T_norm, X[:,0])\n",
    "    param_p_norm = np.append(param_p_norm, X[:,1])\n",
    "    param_x_H2_0_norm = np.append(param_x_H2_0_norm, X[:,2])\n",
    "    param_x_N2_0_norm = np.append(param_x_N2_0_norm, X[:,3])\n",
    "    param_x_NH3_0_norm = np.append(param_x_NH3_0_norm, X[:,4])\n",
    "\n",
    "# print('x_H2:', x_H2_real_norm) #, x_H2_real_norm.dtype())\n",
    "# print('x_H2_pred:', x_H2_pred_norm)\n",
    "x_H2_pred = x_H2_pred_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_real = x_H2_real_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_pred = x_NH3_pred_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_real = x_NH3_real_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "mistake_H2 = abs(x_H2_real - x_H2_pred)\n",
    "mistake_NH3 = abs(x_NH3_real - x_NH3_pred)\n",
    "\n",
    "param_T = param_T_norm * std_in[0].numpy() + mean_in[0].numpy()\n",
    "param_p = param_p_norm * std_in[1].numpy() + mean_in[1].numpy()\n",
    "param_x_H2_0 = param_x_H2_0_norm * std_in[2].numpy() + mean_in[2].numpy()\n",
    "param_x_N2_0 = param_x_N2_0_norm * std_in[3].numpy() + mean_in[3].numpy()\n",
    "param_x_NH3_0 = param_x_NH3_0_norm * std_in[4].numpy() + mean_in[4].numpy()\n",
    "\n",
    "# print('T:', param_T[0])\n",
    "# print(len(param_T))\n",
    "# print(param_T[0])\n",
    "\n",
    "fig,ax = plt.subplots(2,2, figsize = (10, 7)) #gridspec_kw={'width_ratios': [1,1,1,1]})\n",
    "\n",
    "ax[0,0].semilogy(param_T, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0,0].semilogy(param_T, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0,0].set(xlabel = '$T$ / K', ylabel = '|$x\\mathregular{_i} - x\\mathregular{_{i,pred}}$| / mol')\n",
    "ax[0,0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].semilogy(param_p, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0,1].semilogy(param_p, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0,1].set(xlabel = '$p$ / bar', ylabel = '|$x\\mathregular{_i} - x\\mathregular{_{i,pred}}$| / mol')\n",
    "ax[0,1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,0].semilogy(param_x_H2_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2, 0}}$')\n",
    "ax[1,0].semilogy(param_x_N2_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{N_2, 0}}$')\n",
    "ax[1,0].semilogy(param_x_NH3_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{NH_3, 0}}$')\n",
    "ax[1,0].set(xlabel = '$x\\mathregular{_{i,0}}$ / 1', ylabel = '|$x\\mathregular{_{H_2}} - x\\mathregular{_{H_2,pred}}$| / mol')\n",
    "ax[1,0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1,0].set(xlim = (0,1))\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].semilogy(param_x_H2_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{H_2, 0}}$')\n",
    "ax[1,1].semilogy(param_x_N2_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{N_2, 0}}$')\n",
    "ax[1,1].semilogy(param_x_NH3_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3, 0}}$')\n",
    "ax[1,1].set(xlabel = '$x\\mathregular{_{i,0}}$ / 1', ylabel = '|$x\\mathregular{_{NH_3}} - x\\mathregular{_{NH_3,pred}}$| / mol')\n",
    "ax[1,1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1,1].set(xlim = (0,1))\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b5833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#fig.figsize=(12, 8)\n",
    "ax.plot(train_accuracy, label='train accuracy')\n",
    "ax.plot(test_accuracy, label='test accuracy')\n",
    "plt.title(\"Train and Test Accuracy\")\n",
    "ax.set(xlabel = '$Epochs$ / 1', ylabel = '$Accuracy$ / %') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff0471",
   "metadata": {},
   "source": [
    "#### Laufzeit Gleichgewichtsvorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e60ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred_time = time.process_time()\n",
    "for X, y in train_dataloader:\n",
    "            pred = net(X)\n",
    "pred_time = (time.process_time() - start_pred_time)\n",
    "print('Prediction time:', pred_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418caa55",
   "metadata": {},
   "source": [
    "#### Debugging Hilfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen aller Input X und Output y Daten\n",
    "for (X,y) in train_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print(net(X))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241eab8",
   "metadata": {},
   "source": [
    "#### Einblick in Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b043958",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters()) # zeigt weights, biases, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2,5))\n",
    "print(X)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lnorm = nn.LayerNorm(5)\n",
    "Bnorm = nn.BatchNorm1d(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X,y) in train_dataloader:\n",
    "    print(X)\n",
    "    #print(y.reshape((-1,1)))\n",
    "    print(Bnorm(X).mean(dim=0))\n",
    "    print(Bnorm(X))\n",
    "    print(Lnorm(X))\n",
    "    #print((Lnorm(X.permute(0,2,1))).permute(0,2,1))\n",
    "    print(Lnorm(X).mean(dim=0))\n",
    "    print(Lnorm(X).mean(dim=1))\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b867dae",
   "metadata": {},
   "source": [
    "#### Histogramme Verteilung von $xi$ und $x{_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(xi)\n",
    "plt.hist(x_0[:,0],bins=100)\n",
    "plt.hist(x_0[:,1],bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c529b97",
   "metadata": {},
   "source": [
    "#### Speichern des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "'torch.save(net.state_dict(),'data/models/ann_005_022.pth')\n",
    "np.'savez('data/models/params_005_022.npz', mean_in = mean_in, std_in = std_in, mean_out = mean_out, std_out = std_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
