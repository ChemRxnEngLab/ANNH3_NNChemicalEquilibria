{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54dc0a0",
   "metadata": {},
   "source": [
    "# Architektur Neuronales Netz, Output x_H2 und x_NH3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250c18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe / Bibliotheken\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import normalize as norm\n",
    "from torch import log10\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import max_error\n",
    "# from sklearn.metrics import mean_squared_error as MSE\n",
    "# from sklearn.metrics import mean_absolute_error as MAE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9ebf3",
   "metadata": {},
   "source": [
    "#### Default Datentyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68df48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e5067",
   "metadata": {},
   "source": [
    "#### Erzeugnung des Moduls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bffc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    #Initalisierung der Netzwerk layers\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, hidden4_size, hidden5_size, output_size):\n",
    "    \n",
    "        super().__init__() #Referenz zur Base Class (nn.Module)\n",
    "        #Kaskade der Layer\n",
    "        self.linear_afunc_stack = nn.Sequential(\n",
    "            #nn.BatchNorm1d(input_size), # Normalisierung, damit Inputdaten gleiche Größenordnung haben\n",
    "            nn.Linear(input_size, hidden1_size), #Lineare Transformation mit gespeicherten weights und biases\n",
    "            nn.GELU(), #Nicht lineare Aktivierungsfunktion um komplexe nichtlineare Zusammenhänge abzubilden\n",
    "            nn.Linear(hidden1_size, hidden2_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden2_size, hidden3_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden3_size, hidden4_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden4_size, hidden5_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden5_size, output_size),\n",
    "        )\n",
    "\n",
    "    #Implementierung der Operationen auf Input Daten\n",
    "    def forward(self, x):\n",
    "        out = self.linear_afunc_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9ae53",
   "metadata": {},
   "source": [
    "#### Ausgabe Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_afunc_stack): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=200, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (3): GELU(approximate='none')\n",
      "    (4): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (5): GELU(approximate='none')\n",
      "    (6): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (7): GELU(approximate='none')\n",
      "    (8): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (9): GELU(approximate='none')\n",
      "    (10): Linear(in_features=200, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(5, 200, 200, 200, 200, 200, 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1d6ae",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08ff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #Zahl der Datenpaare die vor einem erneuten Update der Parameter ins Netzt gegeben werden\n",
    "eq_data_file = Path.cwd() / 'data' / 'eq_dataset_x.npz' #Import der GGW Daten\n",
    "\n",
    "res = np.load(eq_data_file)\n",
    "\n",
    "# Bei Speicherung wurden Daten als T, p, x_0 und xi gespeichert\n",
    "# Inputs T, p, x_0[H2,N2,NH3]\n",
    "# Outputs x[H2,N2,NH3]\n",
    "# Umwandlen der np.arrays in torch.tensors zur besseren Arbeit mit PyTorch\n",
    "T = torch.tensor(res['T'])\n",
    "p = torch.tensor(res['p'])\n",
    "x_0 = torch.tensor(res['x_0'])\n",
    "x = torch.tensor(res['x'])\n",
    "\n",
    "#Anpassen der Daten auf gleiche Größenordnung\n",
    "#T = log10(T)\n",
    "# T = T / 850\n",
    "# p = p / 1000\n",
    "\n",
    "\n",
    "# print(T.dtype)\n",
    "# print(xi.dtype)\n",
    "\n",
    "x_input = torch.stack((T, p ,x_0[:,0],x_0[:,1],x_0[:,2]),1)\n",
    "y_output = torch.stack((x[:,0], x[:,2]), 1) # [H2, NH3], dritter Stoffmengenanteil ergibt sich den anderen\n",
    "#print(x_input.size())\n",
    "# print(xi.size())\n",
    "\n",
    "# Split des Datensatzes in Trainings und Testdaten\n",
    "split = 0.8 # Anteil Trainingsdaten\n",
    "\n",
    "x_input_train = x_input[:int(split * len(x_input)), :]\n",
    "y_output_train = y_output[:int(split * len(y_output)), :]\n",
    "x_input_test = x_input[int(split * len(x_input)):, :]\n",
    "y_output_test = y_output[int(split * len(y_output)):, :]\n",
    "\n",
    "# Preprocessing Normalisierung der Daten\n",
    "mean_in = torch.mean(x_input_train,0) # Mittelwert\n",
    "std_in = torch.std(x_input_train,0) # Standardabweichung\n",
    "mean_out = torch.mean(y_output_train,0)\n",
    "std_out = torch.std(y_output_train,0)\n",
    "\n",
    "x_input_train_norm = (x_input_train - mean_in) / std_in\n",
    "y_output_train_norm = (y_output_train - mean_out) / std_out\n",
    "\n",
    "x_input_test_norm = (x_input_test - mean_in) / std_in\n",
    "y_output_test_norm = (y_output_test - mean_out) / std_out\n",
    "\n",
    "# print(x_input_train_norm)\n",
    "# print(torch.mean(x_input_train_norm[:,0]))\n",
    "\n",
    "# Tensoren zu einem großen Set gruppieren\n",
    "train_dataset = TensorDataset(x_input_train_norm, y_output_train_norm)\n",
    "test_dataset = TensorDataset(x_input_test_norm, y_output_test_norm)\n",
    "    \n",
    "# # Split in Trainings und Test Set\n",
    "# train_dataset, test_dataset = random_split(dataset, \n",
    "#                                            [int(0.8*len(dataset)), int(0.2*len(dataset))], # splitting 80/20\n",
    "#                                            generator = torch.Generator().manual_seed(42)) # Festlegung seed zur Reproduktivität\n",
    "\n",
    "# Erzeugen der DataLoader zur Arbeit mit Daten\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True) # shuffle batches zur Reduzierung von overfitting\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e9841",
   "metadata": {},
   "source": [
    "#### Generierung Netzwerk, Festlegung von loss Funktion und Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ab5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung aNN\n",
    "net = NeuralNetwork(5, 200, 200, 200, 200, 200, 2)\n",
    "\n",
    "# Loss Funktion; gibt Fehler an\n",
    "loss_fn_MSE = nn.MSELoss()\n",
    "loss_fn_L1 = nn.L1Loss()\n",
    "\n",
    "#Definition custom loss Funktion, MRE\n",
    "def MRELoss(outputs, targets):\n",
    "    \n",
    "    loss = torch.mean(abs((outputs - targets) / targets))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "#Optimizer\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "#scheduler = StepLR(optimizer, step_size = 30, gamma = 0.1)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[30, 70, 100], gamma = 0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 10, threshold = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ccc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-6\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b61b7",
   "metadata": {},
   "source": [
    "#### Funktion zur Bestimmung der Genauigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4480b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, net):\n",
    "    \n",
    "    loss = 0\n",
    "    MRE = 0\n",
    "    MAE = 0\n",
    "    train_correct = 0\n",
    "    train_total = len(loader.dataset)\n",
    "    num_batches = len(loader) \n",
    "    #train_total = 0\n",
    "    \n",
    "    net.eval() # Put network in evaluation mode\n",
    "    \n",
    "    if loader == train_dataloader:\n",
    "        dataset = \"Train\"\n",
    "    else:\n",
    "        dataset = \"Test\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            pred = net(X)\n",
    "           \n",
    "            #loss += MRELoss(pred, y).item()\n",
    "            loss += loss_fn_MSE(pred, y) # Calculate the loss\n",
    "            MRE += MRELoss(pred * std_out + mean_out, y * std_out + mean_out)\n",
    "            MAE += loss_fn_L1(pred * std_out + mean_out, y * std_out + mean_out)\n",
    "            \n",
    "            # Record the correct predictions for training data\n",
    "            #_, predictions = torch.max(pred.data, 1)\n",
    "            for i in range(len(pred)):\n",
    "                if ((pred[i,0] * std_out[0] + mean_out[0]) - (y[i,0] * std_out[0] + mean_out[0]) and (pred[i,1] * std_out[1] + mean_out[1]) - (y[i,1] * std_out[1] + mean_out[1])) <= 0.01:\n",
    "                    train_correct += 1\n",
    "            #train_correct += (abs(pred.argmax(1) - y) <= 0.01).sum().item()\n",
    "            #train_correct += (abs(predictions - y.data) <= 0.01).sum()\n",
    "            #train_total += predictions.size(0)\n",
    "            \n",
    "        # Genauigkeit berechnen\n",
    "        acc = float(train_correct) / float(train_total) * 100\n",
    "        acc = round(acc, 2)\n",
    "        \n",
    "        loss /= num_batches\n",
    "        MRE /= num_batches\n",
    "        MAE /= num_batches\n",
    "\n",
    "        print(f\"{dataset} Error: \\n Accuracy: {acc}%, Avg loss: {loss:>8f}, MRE: {MRE:>8f}, MAE: {MAE:>8f} \\n\")\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    return acc, loss, MRE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd049ed",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771789d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Iteration 1/12, Loss: 0.8666\n",
      "Epoch 1/200, Iteration 2/12, Loss: 1.0961\n",
      "Epoch 1/200, Iteration 3/12, Loss: 0.5424\n",
      "Epoch 1/200, Iteration 4/12, Loss: 0.6023\n",
      "Epoch 1/200, Iteration 5/12, Loss: 0.4318\n",
      "Epoch 1/200, Iteration 6/12, Loss: 0.5661\n",
      "Epoch 1/200, Iteration 7/12, Loss: 0.5476\n",
      "Epoch 1/200, Iteration 8/12, Loss: 0.4208\n",
      "Epoch 1/200, Iteration 9/12, Loss: 0.2690\n",
      "Epoch 1/200, Iteration 10/12, Loss: 0.1854\n",
      "Epoch 1/200, Iteration 11/12, Loss: 0.3609\n",
      "Epoch 1/200, Iteration 12/12, Loss: 0.1100\n",
      "Epoch 1/200, Iteration 13/12, Loss: 0.2036\n",
      "Train Error: \n",
      " Accuracy: 64.88%, Avg loss: 0.181780, MRE: 0.238041, MAE: 0.041951 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.170250, MRE: 0.202609, MAE: 0.037899 \n",
      "\n",
      "Epoch 2/200, Iteration 1/12, Loss: 0.2150\n",
      "Epoch 2/200, Iteration 2/12, Loss: 0.1063\n",
      "Epoch 2/200, Iteration 3/12, Loss: 0.1674\n",
      "Epoch 2/200, Iteration 4/12, Loss: 0.1423\n",
      "Epoch 2/200, Iteration 5/12, Loss: 0.1170\n",
      "Epoch 2/200, Iteration 6/12, Loss: 0.1037\n",
      "Epoch 2/200, Iteration 7/12, Loss: 0.0860\n",
      "Epoch 2/200, Iteration 8/12, Loss: 0.0675\n",
      "Epoch 2/200, Iteration 9/12, Loss: 0.1044\n",
      "Epoch 2/200, Iteration 10/12, Loss: 0.0643\n",
      "Epoch 2/200, Iteration 11/12, Loss: 0.0767\n",
      "Epoch 2/200, Iteration 12/12, Loss: 0.0924\n",
      "Epoch 2/200, Iteration 13/12, Loss: 0.0942\n",
      "Train Error: \n",
      " Accuracy: 53.37%, Avg loss: 0.059660, MRE: 0.184379, MAE: 0.025835 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.5%, Avg loss: 0.059797, MRE: 0.158180, MAE: 0.026290 \n",
      "\n",
      "Epoch 3/200, Iteration 1/12, Loss: 0.0735\n",
      "Epoch 3/200, Iteration 2/12, Loss: 0.0444\n",
      "Epoch 3/200, Iteration 3/12, Loss: 0.0619\n",
      "Epoch 3/200, Iteration 4/12, Loss: 0.0732\n",
      "Epoch 3/200, Iteration 5/12, Loss: 0.0427\n",
      "Epoch 3/200, Iteration 6/12, Loss: 0.0597\n",
      "Epoch 3/200, Iteration 7/12, Loss: 0.0964\n",
      "Epoch 3/200, Iteration 8/12, Loss: 0.0223\n",
      "Epoch 3/200, Iteration 9/12, Loss: 0.0354\n",
      "Epoch 3/200, Iteration 10/12, Loss: 0.0619\n",
      "Epoch 3/200, Iteration 11/12, Loss: 0.0679\n",
      "Epoch 3/200, Iteration 12/12, Loss: 0.0495\n",
      "Epoch 3/200, Iteration 13/12, Loss: 0.0782\n",
      "Train Error: \n",
      " Accuracy: 63.88%, Avg loss: 0.075189, MRE: 0.121356, MAE: 0.021919 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 0.069451, MRE: 0.116762, MAE: 0.022715 \n",
      "\n",
      "Epoch 4/200, Iteration 1/12, Loss: 0.0596\n",
      "Epoch 4/200, Iteration 2/12, Loss: 0.0711\n",
      "Epoch 4/200, Iteration 3/12, Loss: 0.0415\n",
      "Epoch 4/200, Iteration 4/12, Loss: 0.0673\n",
      "Epoch 4/200, Iteration 5/12, Loss: 0.0346\n",
      "Epoch 4/200, Iteration 6/12, Loss: 0.0430\n",
      "Epoch 4/200, Iteration 7/12, Loss: 0.0800\n",
      "Epoch 4/200, Iteration 8/12, Loss: 0.0531\n",
      "Epoch 4/200, Iteration 9/12, Loss: 0.0255\n",
      "Epoch 4/200, Iteration 10/12, Loss: 0.0567\n",
      "Epoch 4/200, Iteration 11/12, Loss: 0.0385\n",
      "Epoch 4/200, Iteration 12/12, Loss: 0.0276\n",
      "Epoch 4/200, Iteration 13/12, Loss: 0.0267\n",
      "Train Error: \n",
      " Accuracy: 63.25%, Avg loss: 0.030524, MRE: 0.113566, MAE: 0.017153 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.022837, MRE: 0.089490, MAE: 0.015598 \n",
      "\n",
      "Epoch 5/200, Iteration 1/12, Loss: 0.0263\n",
      "Epoch 5/200, Iteration 2/12, Loss: 0.0263\n",
      "Epoch 5/200, Iteration 3/12, Loss: 0.0170\n",
      "Epoch 5/200, Iteration 4/12, Loss: 0.0200\n",
      "Epoch 5/200, Iteration 5/12, Loss: 0.0195\n",
      "Epoch 5/200, Iteration 6/12, Loss: 0.0162\n",
      "Epoch 5/200, Iteration 7/12, Loss: 0.0179\n",
      "Epoch 5/200, Iteration 8/12, Loss: 0.0175\n",
      "Epoch 5/200, Iteration 9/12, Loss: 0.0188\n",
      "Epoch 5/200, Iteration 10/12, Loss: 0.0190\n",
      "Epoch 5/200, Iteration 11/12, Loss: 0.0110\n",
      "Epoch 5/200, Iteration 12/12, Loss: 0.0142\n",
      "Epoch 5/200, Iteration 13/12, Loss: 0.0162\n",
      "Train Error: \n",
      " Accuracy: 88.75%, Avg loss: 0.016671, MRE: 0.081294, MAE: 0.014417 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.015919, MRE: 0.067498, MAE: 0.013782 \n",
      "\n",
      "Epoch 6/200, Iteration 1/12, Loss: 0.0158\n",
      "Epoch 6/200, Iteration 2/12, Loss: 0.0180\n",
      "Epoch 6/200, Iteration 3/12, Loss: 0.0096\n",
      "Epoch 6/200, Iteration 4/12, Loss: 0.0135\n",
      "Epoch 6/200, Iteration 5/12, Loss: 0.0128\n",
      "Epoch 6/200, Iteration 6/12, Loss: 0.0106\n",
      "Epoch 6/200, Iteration 7/12, Loss: 0.0127\n",
      "Epoch 6/200, Iteration 8/12, Loss: 0.0085\n",
      "Epoch 6/200, Iteration 9/12, Loss: 0.0146\n",
      "Epoch 6/200, Iteration 10/12, Loss: 0.0093\n",
      "Epoch 6/200, Iteration 11/12, Loss: 0.0160\n",
      "Epoch 6/200, Iteration 12/12, Loss: 0.0097\n",
      "Epoch 6/200, Iteration 13/12, Loss: 0.0080\n",
      "Train Error: \n",
      " Accuracy: 93.12%, Avg loss: 0.010559, MRE: 0.062699, MAE: 0.011469 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.009878, MRE: 0.054081, MAE: 0.011074 \n",
      "\n",
      "Epoch 7/200, Iteration 1/12, Loss: 0.0093\n",
      "Epoch 7/200, Iteration 2/12, Loss: 0.0102\n",
      "Epoch 7/200, Iteration 3/12, Loss: 0.0040\n",
      "Epoch 7/200, Iteration 4/12, Loss: 0.0088\n",
      "Epoch 7/200, Iteration 5/12, Loss: 0.0078\n",
      "Epoch 7/200, Iteration 6/12, Loss: 0.0081\n",
      "Epoch 7/200, Iteration 7/12, Loss: 0.0045\n",
      "Epoch 7/200, Iteration 8/12, Loss: 0.0071\n",
      "Epoch 7/200, Iteration 9/12, Loss: 0.0062\n",
      "Epoch 7/200, Iteration 10/12, Loss: 0.0054\n",
      "Epoch 7/200, Iteration 11/12, Loss: 0.0041\n",
      "Epoch 7/200, Iteration 12/12, Loss: 0.0044\n",
      "Epoch 7/200, Iteration 13/12, Loss: 0.0051\n",
      "Train Error: \n",
      " Accuracy: 97.38%, Avg loss: 0.003772, MRE: 0.034491, MAE: 0.006444 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.003595, MRE: 0.026963, MAE: 0.006302 \n",
      "\n",
      "Epoch 8/200, Iteration 1/12, Loss: 0.0051\n",
      "Epoch 8/200, Iteration 2/12, Loss: 0.0037\n",
      "Epoch 8/200, Iteration 3/12, Loss: 0.0024\n",
      "Epoch 8/200, Iteration 4/12, Loss: 0.0039\n",
      "Epoch 8/200, Iteration 5/12, Loss: 0.0030\n",
      "Epoch 8/200, Iteration 6/12, Loss: 0.0029\n",
      "Epoch 8/200, Iteration 7/12, Loss: 0.0020\n",
      "Epoch 8/200, Iteration 8/12, Loss: 0.0015\n",
      "Epoch 8/200, Iteration 9/12, Loss: 0.0026\n",
      "Epoch 8/200, Iteration 10/12, Loss: 0.0022\n",
      "Epoch 8/200, Iteration 11/12, Loss: 0.0018\n",
      "Epoch 8/200, Iteration 12/12, Loss: 0.0017\n",
      "Epoch 8/200, Iteration 13/12, Loss: 0.0018\n",
      "Train Error: \n",
      " Accuracy: 99.12%, Avg loss: 0.001598, MRE: 0.023854, MAE: 0.004024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.001576, MRE: 0.022642, MAE: 0.003966 \n",
      "\n",
      "Epoch 9/200, Iteration 1/12, Loss: 0.0015\n",
      "Epoch 9/200, Iteration 2/12, Loss: 0.0020\n",
      "Epoch 9/200, Iteration 3/12, Loss: 0.0020\n",
      "Epoch 9/200, Iteration 4/12, Loss: 0.0009\n",
      "Epoch 9/200, Iteration 5/12, Loss: 0.0021\n",
      "Epoch 9/200, Iteration 6/12, Loss: 0.0014\n",
      "Epoch 9/200, Iteration 7/12, Loss: 0.0016\n",
      "Epoch 9/200, Iteration 8/12, Loss: 0.0017\n",
      "Epoch 9/200, Iteration 9/12, Loss: 0.0017\n",
      "Epoch 9/200, Iteration 10/12, Loss: 0.0013\n",
      "Epoch 9/200, Iteration 11/12, Loss: 0.0015\n",
      "Epoch 9/200, Iteration 12/12, Loss: 0.0013\n",
      "Epoch 9/200, Iteration 13/12, Loss: 0.0015\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.001809, MRE: 0.021538, MAE: 0.004753 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.001666, MRE: 0.019504, MAE: 0.004466 \n",
      "\n",
      "Epoch 10/200, Iteration 1/12, Loss: 0.0018\n",
      "Epoch 10/200, Iteration 2/12, Loss: 0.0014\n",
      "Epoch 10/200, Iteration 3/12, Loss: 0.0020\n",
      "Epoch 10/200, Iteration 4/12, Loss: 0.0008\n",
      "Epoch 10/200, Iteration 5/12, Loss: 0.0006\n",
      "Epoch 10/200, Iteration 6/12, Loss: 0.0020\n",
      "Epoch 10/200, Iteration 7/12, Loss: 0.0013\n",
      "Epoch 10/200, Iteration 8/12, Loss: 0.0008\n",
      "Epoch 10/200, Iteration 9/12, Loss: 0.0012\n",
      "Epoch 10/200, Iteration 10/12, Loss: 0.0016\n",
      "Epoch 10/200, Iteration 11/12, Loss: 0.0009\n",
      "Epoch 10/200, Iteration 12/12, Loss: 0.0014\n",
      "Epoch 10/200, Iteration 13/12, Loss: 0.0008\n",
      "Train Error: \n",
      " Accuracy: 98.12%, Avg loss: 0.001240, MRE: 0.020765, MAE: 0.003365 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000800, MRE: 0.015200, MAE: 0.003262 \n",
      "\n",
      "Epoch 11/200, Iteration 1/12, Loss: 0.0008\n",
      "Epoch 11/200, Iteration 2/12, Loss: 0.0012\n",
      "Epoch 11/200, Iteration 3/12, Loss: 0.0018\n",
      "Epoch 11/200, Iteration 4/12, Loss: 0.0020\n",
      "Epoch 11/200, Iteration 5/12, Loss: 0.0010\n",
      "Epoch 11/200, Iteration 6/12, Loss: 0.0034\n",
      "Epoch 11/200, Iteration 7/12, Loss: 0.0015\n",
      "Epoch 11/200, Iteration 8/12, Loss: 0.0031\n",
      "Epoch 11/200, Iteration 9/12, Loss: 0.0015\n",
      "Epoch 11/200, Iteration 10/12, Loss: 0.0013\n",
      "Epoch 11/200, Iteration 11/12, Loss: 0.0024\n",
      "Epoch 11/200, Iteration 12/12, Loss: 0.0020\n",
      "Epoch 11/200, Iteration 13/12, Loss: 0.0018\n",
      "Train Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.001953, MRE: 0.021489, MAE: 0.004566 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.002206, MRE: 0.022739, MAE: 0.004987 \n",
      "\n",
      "Epoch 12/200, Iteration 1/12, Loss: 0.0019\n",
      "Epoch 12/200, Iteration 2/12, Loss: 0.0017\n",
      "Epoch 12/200, Iteration 3/12, Loss: 0.0013\n",
      "Epoch 12/200, Iteration 4/12, Loss: 0.0009\n",
      "Epoch 12/200, Iteration 5/12, Loss: 0.0007\n",
      "Epoch 12/200, Iteration 6/12, Loss: 0.0013\n",
      "Epoch 12/200, Iteration 7/12, Loss: 0.0010\n",
      "Epoch 12/200, Iteration 8/12, Loss: 0.0012\n",
      "Epoch 12/200, Iteration 9/12, Loss: 0.0019\n",
      "Epoch 12/200, Iteration 10/12, Loss: 0.0011\n",
      "Epoch 12/200, Iteration 11/12, Loss: 0.0018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200, Iteration 12/12, Loss: 0.0013\n",
      "Epoch 12/200, Iteration 13/12, Loss: 0.0013\n",
      "Train Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.001931, MRE: 0.023134, MAE: 0.005117 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.002317, MRE: 0.023220, MAE: 0.005525 \n",
      "\n",
      "Epoch 13/200, Iteration 1/12, Loss: 0.0021\n",
      "Epoch 13/200, Iteration 2/12, Loss: 0.0011\n",
      "Epoch 13/200, Iteration 3/12, Loss: 0.0016\n",
      "Epoch 13/200, Iteration 4/12, Loss: 0.0011\n",
      "Epoch 13/200, Iteration 5/12, Loss: 0.0016\n",
      "Epoch 13/200, Iteration 6/12, Loss: 0.0008\n",
      "Epoch 13/200, Iteration 7/12, Loss: 0.0007\n",
      "Epoch 13/200, Iteration 8/12, Loss: 0.0014\n",
      "Epoch 13/200, Iteration 9/12, Loss: 0.0011\n",
      "Epoch 13/200, Iteration 10/12, Loss: 0.0010\n",
      "Epoch 13/200, Iteration 11/12, Loss: 0.0012\n",
      "Epoch 13/200, Iteration 12/12, Loss: 0.0004\n",
      "Epoch 13/200, Iteration 13/12, Loss: 0.0012\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000797, MRE: 0.014784, MAE: 0.002669 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000711, MRE: 0.011174, MAE: 0.002581 \n",
      "\n",
      "Epoch 14/200, Iteration 1/12, Loss: 0.0005\n",
      "Epoch 14/200, Iteration 2/12, Loss: 0.0022\n",
      "Epoch 14/200, Iteration 3/12, Loss: 0.0012\n",
      "Epoch 14/200, Iteration 4/12, Loss: 0.0011\n",
      "Epoch 14/200, Iteration 5/12, Loss: 0.0008\n",
      "Epoch 14/200, Iteration 6/12, Loss: 0.0012\n",
      "Epoch 14/200, Iteration 7/12, Loss: 0.0009\n",
      "Epoch 14/200, Iteration 8/12, Loss: 0.0013\n",
      "Epoch 14/200, Iteration 9/12, Loss: 0.0008\n",
      "Epoch 14/200, Iteration 10/12, Loss: 0.0007\n",
      "Epoch 14/200, Iteration 11/12, Loss: 0.0006\n",
      "Epoch 14/200, Iteration 12/12, Loss: 0.0005\n",
      "Epoch 14/200, Iteration 13/12, Loss: 0.0010\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000915, MRE: 0.024677, MAE: 0.003032 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000964, MRE: 0.019450, MAE: 0.003299 \n",
      "\n",
      "Epoch 15/200, Iteration 1/12, Loss: 0.0011\n",
      "Epoch 15/200, Iteration 2/12, Loss: 0.0014\n",
      "Epoch 15/200, Iteration 3/12, Loss: 0.0008\n",
      "Epoch 15/200, Iteration 4/12, Loss: 0.0009\n",
      "Epoch 15/200, Iteration 5/12, Loss: 0.0009\n",
      "Epoch 15/200, Iteration 6/12, Loss: 0.0006\n",
      "Epoch 15/200, Iteration 7/12, Loss: 0.0007\n",
      "Epoch 15/200, Iteration 8/12, Loss: 0.0014\n",
      "Epoch 15/200, Iteration 9/12, Loss: 0.0015\n",
      "Epoch 15/200, Iteration 10/12, Loss: 0.0011\n",
      "Epoch 15/200, Iteration 11/12, Loss: 0.0019\n",
      "Epoch 15/200, Iteration 12/12, Loss: 0.0014\n",
      "Epoch 15/200, Iteration 13/12, Loss: 0.0016\n",
      "Train Error: \n",
      " Accuracy: 98.25%, Avg loss: 0.001650, MRE: 0.026970, MAE: 0.003504 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000966, MRE: 0.019067, MAE: 0.003244 \n",
      "\n",
      "Epoch 16/200, Iteration 1/12, Loss: 0.0026\n",
      "Epoch 16/200, Iteration 2/12, Loss: 0.0013\n",
      "Epoch 16/200, Iteration 3/12, Loss: 0.0018\n",
      "Epoch 16/200, Iteration 4/12, Loss: 0.0026\n",
      "Epoch 16/200, Iteration 5/12, Loss: 0.0030\n",
      "Epoch 16/200, Iteration 6/12, Loss: 0.0014\n",
      "Epoch 16/200, Iteration 7/12, Loss: 0.0018\n",
      "Epoch 16/200, Iteration 8/12, Loss: 0.0025\n",
      "Epoch 16/200, Iteration 9/12, Loss: 0.0016\n",
      "Epoch 16/200, Iteration 10/12, Loss: 0.0012\n",
      "Epoch 16/200, Iteration 11/12, Loss: 0.0018\n",
      "Epoch 16/200, Iteration 12/12, Loss: 0.0017\n",
      "Epoch 16/200, Iteration 13/12, Loss: 0.0015\n",
      "Train Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.001047, MRE: 0.017228, MAE: 0.003514 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000841, MRE: 0.013993, MAE: 0.003286 \n",
      "\n",
      "Epoch 17/200, Iteration 1/12, Loss: 0.0008\n",
      "Epoch 17/200, Iteration 2/12, Loss: 0.0020\n",
      "Epoch 17/200, Iteration 3/12, Loss: 0.0035\n",
      "Epoch 17/200, Iteration 4/12, Loss: 0.0033\n",
      "Epoch 17/200, Iteration 5/12, Loss: 0.0027\n",
      "Epoch 17/200, Iteration 6/12, Loss: 0.0050\n",
      "Epoch 17/200, Iteration 7/12, Loss: 0.0020\n",
      "Epoch 17/200, Iteration 8/12, Loss: 0.0042\n",
      "Epoch 17/200, Iteration 9/12, Loss: 0.0017\n",
      "Epoch 17/200, Iteration 10/12, Loss: 0.0056\n",
      "Epoch 17/200, Iteration 11/12, Loss: 0.0021\n",
      "Epoch 17/200, Iteration 12/12, Loss: 0.0030\n",
      "Epoch 17/200, Iteration 13/12, Loss: 0.0018\n",
      "Train Error: \n",
      " Accuracy: 87.12%, Avg loss: 0.003337, MRE: 0.028527, MAE: 0.005157 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.003165, MRE: 0.023526, MAE: 0.005110 \n",
      "\n",
      "Epoch 18/200, Iteration 1/12, Loss: 0.0027\n",
      "Epoch 18/200, Iteration 2/12, Loss: 0.0028\n",
      "Epoch 18/200, Iteration 3/12, Loss: 0.0012\n",
      "Epoch 18/200, Iteration 4/12, Loss: 0.0059\n",
      "Epoch 18/200, Iteration 5/12, Loss: 0.0026\n",
      "Epoch 18/200, Iteration 6/12, Loss: 0.0038\n",
      "Epoch 18/200, Iteration 7/12, Loss: 0.0059\n",
      "Epoch 18/200, Iteration 8/12, Loss: 0.0055\n",
      "Epoch 18/200, Iteration 9/12, Loss: 0.0060\n",
      "Epoch 18/200, Iteration 10/12, Loss: 0.0036\n",
      "Epoch 18/200, Iteration 11/12, Loss: 0.0096\n",
      "Epoch 18/200, Iteration 12/12, Loss: 0.0071\n",
      "Epoch 18/200, Iteration 13/12, Loss: 0.0062\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.017736, MRE: 0.103830, MAE: 0.013037 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.014677, MRE: 0.079565, MAE: 0.011284 \n",
      "\n",
      "Epoch 19/200, Iteration 1/12, Loss: 0.0176\n",
      "Epoch 19/200, Iteration 2/12, Loss: 0.0067\n",
      "Epoch 19/200, Iteration 3/12, Loss: 0.0078\n",
      "Epoch 19/200, Iteration 4/12, Loss: 0.0106\n",
      "Epoch 19/200, Iteration 5/12, Loss: 0.0076\n",
      "Epoch 19/200, Iteration 6/12, Loss: 0.0163\n",
      "Epoch 19/200, Iteration 7/12, Loss: 0.0104\n",
      "Epoch 19/200, Iteration 8/12, Loss: 0.0352\n",
      "Epoch 19/200, Iteration 9/12, Loss: 0.0105\n",
      "Epoch 19/200, Iteration 10/12, Loss: 0.0349\n",
      "Epoch 19/200, Iteration 11/12, Loss: 0.0187\n",
      "Epoch 19/200, Iteration 12/12, Loss: 0.0174\n",
      "Epoch 19/200, Iteration 13/12, Loss: 0.0317\n",
      "Train Error: \n",
      " Accuracy: 96.62%, Avg loss: 0.019012, MRE: 0.076425, MAE: 0.016814 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.020490, MRE: 0.067827, MAE: 0.017159 \n",
      "\n",
      "Epoch 20/200, Iteration 1/12, Loss: 0.0190\n",
      "Epoch 20/200, Iteration 2/12, Loss: 0.0296\n",
      "Epoch 20/200, Iteration 3/12, Loss: 0.0090\n",
      "Epoch 20/200, Iteration 4/12, Loss: 0.0237\n",
      "Epoch 20/200, Iteration 5/12, Loss: 0.0287\n",
      "Epoch 20/200, Iteration 6/12, Loss: 0.0121\n",
      "Epoch 20/200, Iteration 7/12, Loss: 0.0140\n",
      "Epoch 20/200, Iteration 8/12, Loss: 0.0155\n",
      "Epoch 20/200, Iteration 9/12, Loss: 0.0109\n",
      "Epoch 20/200, Iteration 10/12, Loss: 0.0321\n",
      "Epoch 20/200, Iteration 11/12, Loss: 0.0074\n",
      "Epoch 20/200, Iteration 12/12, Loss: 0.0205\n",
      "Epoch 20/200, Iteration 13/12, Loss: 0.0215\n",
      "Train Error: \n",
      " Accuracy: 56.62%, Avg loss: 0.023364, MRE: 0.080243, MAE: 0.011579 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: 0.020519, MRE: 0.075574, MAE: 0.012133 \n",
      "\n",
      "Epoch 21/200, Iteration 1/12, Loss: 0.0155\n",
      "Epoch 21/200, Iteration 2/12, Loss: 0.0241\n",
      "Epoch 21/200, Iteration 3/12, Loss: 0.0172\n",
      "Epoch 21/200, Iteration 4/12, Loss: 0.0238\n",
      "Epoch 21/200, Iteration 5/12, Loss: 0.0240\n",
      "Epoch 21/200, Iteration 6/12, Loss: 0.0216\n",
      "Epoch 21/200, Iteration 7/12, Loss: 0.0184\n",
      "Epoch 21/200, Iteration 8/12, Loss: 0.0209\n",
      "Epoch 21/200, Iteration 9/12, Loss: 0.0126\n",
      "Epoch 21/200, Iteration 10/12, Loss: 0.0139\n",
      "Epoch 21/200, Iteration 11/12, Loss: 0.0135\n",
      "Epoch 21/200, Iteration 12/12, Loss: 0.0145\n",
      "Epoch 21/200, Iteration 13/12, Loss: 0.0077\n",
      "Train Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.011209, MRE: 0.073888, MAE: 0.010690 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.007874, MRE: 0.055761, MAE: 0.009168 \n",
      "\n",
      "Epoch 22/200, Iteration 1/12, Loss: 0.0100\n",
      "Epoch 22/200, Iteration 2/12, Loss: 0.0109\n",
      "Epoch 22/200, Iteration 3/12, Loss: 0.0162\n",
      "Epoch 22/200, Iteration 4/12, Loss: 0.0132\n",
      "Epoch 22/200, Iteration 5/12, Loss: 0.0087\n",
      "Epoch 22/200, Iteration 6/12, Loss: 0.0067\n",
      "Epoch 22/200, Iteration 7/12, Loss: 0.0092\n",
      "Epoch 22/200, Iteration 8/12, Loss: 0.0134\n",
      "Epoch 22/200, Iteration 9/12, Loss: 0.0107\n",
      "Epoch 22/200, Iteration 10/12, Loss: 0.0101\n",
      "Epoch 22/200, Iteration 11/12, Loss: 0.0063\n",
      "Epoch 22/200, Iteration 12/12, Loss: 0.0093\n",
      "Epoch 22/200, Iteration 13/12, Loss: 0.0078\n",
      "Train Error: \n",
      " Accuracy: 84.25%, Avg loss: 0.007231, MRE: 0.056582, MAE: 0.008029 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.005605, MRE: 0.046593, MAE: 0.007854 \n",
      "\n",
      "Epoch 23/200, Iteration 1/12, Loss: 0.0094\n",
      "Epoch 23/200, Iteration 2/12, Loss: 0.0031\n",
      "Epoch 23/200, Iteration 3/12, Loss: 0.0033\n",
      "Epoch 23/200, Iteration 4/12, Loss: 0.0062\n",
      "Epoch 23/200, Iteration 5/12, Loss: 0.0097\n",
      "Epoch 23/200, Iteration 6/12, Loss: 0.0058\n",
      "Epoch 23/200, Iteration 7/12, Loss: 0.0068\n",
      "Epoch 23/200, Iteration 8/12, Loss: 0.0120\n",
      "Epoch 23/200, Iteration 9/12, Loss: 0.0068\n",
      "Epoch 23/200, Iteration 10/12, Loss: 0.0082\n",
      "Epoch 23/200, Iteration 11/12, Loss: 0.0074\n",
      "Epoch 23/200, Iteration 12/12, Loss: 0.0211\n",
      "Epoch 23/200, Iteration 13/12, Loss: 0.0040\n",
      "Train Error: \n",
      " Accuracy: 91.62%, Avg loss: 0.007997, MRE: 0.053231, MAE: 0.009539 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.007104, MRE: 0.047728, MAE: 0.009698 \n",
      "\n",
      "Epoch 24/200, Iteration 1/12, Loss: 0.0076\n",
      "Epoch 24/200, Iteration 2/12, Loss: 0.0045\n",
      "Epoch 24/200, Iteration 3/12, Loss: 0.0147\n",
      "Epoch 24/200, Iteration 4/12, Loss: 0.0084\n",
      "Epoch 24/200, Iteration 5/12, Loss: 0.0111\n",
      "Epoch 24/200, Iteration 6/12, Loss: 0.0099\n",
      "Epoch 24/200, Iteration 7/12, Loss: 0.0039\n",
      "Epoch 24/200, Iteration 8/12, Loss: 0.0045\n",
      "Epoch 24/200, Iteration 9/12, Loss: 0.0031\n",
      "Epoch 24/200, Iteration 10/12, Loss: 0.0090\n",
      "Epoch 24/200, Iteration 11/12, Loss: 0.0050\n",
      "Epoch 24/200, Iteration 12/12, Loss: 0.0022\n",
      "Epoch 24/200, Iteration 13/12, Loss: 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 98.25%, Avg loss: 0.004174, MRE: 0.027253, MAE: 0.006353 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.003722, MRE: 0.025308, MAE: 0.006260 \n",
      "\n",
      "Epoch 25/200, Iteration 1/12, Loss: 0.0040\n",
      "Epoch 25/200, Iteration 2/12, Loss: 0.0030\n",
      "Epoch 25/200, Iteration 3/12, Loss: 0.0052\n",
      "Epoch 25/200, Iteration 4/12, Loss: 0.0023\n",
      "Epoch 25/200, Iteration 5/12, Loss: 0.0042\n",
      "Epoch 25/200, Iteration 6/12, Loss: 0.0023\n",
      "Epoch 25/200, Iteration 7/12, Loss: 0.0033\n",
      "Epoch 25/200, Iteration 8/12, Loss: 0.0011\n",
      "Epoch 25/200, Iteration 9/12, Loss: 0.0022\n",
      "Epoch 25/200, Iteration 10/12, Loss: 0.0011\n",
      "Epoch 25/200, Iteration 11/12, Loss: 0.0010\n",
      "Epoch 25/200, Iteration 12/12, Loss: 0.0008\n",
      "Epoch 25/200, Iteration 13/12, Loss: 0.0012\n",
      "Train Error: \n",
      " Accuracy: 99.75%, Avg loss: 0.000886, MRE: 0.017608, MAE: 0.003079 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000943, MRE: 0.018055, MAE: 0.003144 \n",
      "\n",
      "Epoch 26/200, Iteration 1/12, Loss: 0.0009\n",
      "Epoch 26/200, Iteration 2/12, Loss: 0.0009\n",
      "Epoch 26/200, Iteration 3/12, Loss: 0.0009\n",
      "Epoch 26/200, Iteration 4/12, Loss: 0.0008\n",
      "Epoch 26/200, Iteration 5/12, Loss: 0.0009\n",
      "Epoch 26/200, Iteration 6/12, Loss: 0.0008\n",
      "Epoch 26/200, Iteration 7/12, Loss: 0.0008\n",
      "Epoch 26/200, Iteration 8/12, Loss: 0.0012\n",
      "Epoch 26/200, Iteration 9/12, Loss: 0.0005\n",
      "Epoch 26/200, Iteration 10/12, Loss: 0.0006\n",
      "Epoch 26/200, Iteration 11/12, Loss: 0.0009\n",
      "Epoch 26/200, Iteration 12/12, Loss: 0.0008\n",
      "Epoch 26/200, Iteration 13/12, Loss: 0.0009\n",
      "Train Error: \n",
      " Accuracy: 99.5%, Avg loss: 0.000568, MRE: 0.012643, MAE: 0.002397 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000581, MRE: 0.011215, MAE: 0.002529 \n",
      "\n",
      "Epoch 27/200, Iteration 1/12, Loss: 0.0005\n",
      "Epoch 27/200, Iteration 2/12, Loss: 0.0007\n",
      "Epoch 27/200, Iteration 3/12, Loss: 0.0004\n",
      "Epoch 27/200, Iteration 4/12, Loss: 0.0007\n",
      "Epoch 27/200, Iteration 5/12, Loss: 0.0004\n",
      "Epoch 27/200, Iteration 6/12, Loss: 0.0005\n",
      "Epoch 27/200, Iteration 7/12, Loss: 0.0009\n",
      "Epoch 27/200, Iteration 8/12, Loss: 0.0005\n",
      "Epoch 27/200, Iteration 9/12, Loss: 0.0003\n",
      "Epoch 27/200, Iteration 10/12, Loss: 0.0004\n",
      "Epoch 27/200, Iteration 11/12, Loss: 0.0005\n",
      "Epoch 27/200, Iteration 12/12, Loss: 0.0007\n",
      "Epoch 27/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 99.75%, Avg loss: 0.000422, MRE: 0.010851, MAE: 0.002002 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000427, MRE: 0.008714, MAE: 0.002112 \n",
      "\n",
      "Epoch 28/200, Iteration 1/12, Loss: 0.0005\n",
      "Epoch 28/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 28/200, Iteration 3/12, Loss: 0.0004\n",
      "Epoch 28/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 28/200, Iteration 5/12, Loss: 0.0005\n",
      "Epoch 28/200, Iteration 6/12, Loss: 0.0004\n",
      "Epoch 28/200, Iteration 7/12, Loss: 0.0005\n",
      "Epoch 28/200, Iteration 8/12, Loss: 0.0005\n",
      "Epoch 28/200, Iteration 9/12, Loss: 0.0004\n",
      "Epoch 28/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 28/200, Iteration 11/12, Loss: 0.0003\n",
      "Epoch 28/200, Iteration 12/12, Loss: 0.0003\n",
      "Epoch 28/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 99.75%, Avg loss: 0.000327, MRE: 0.009301, MAE: 0.001652 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000329, MRE: 0.007858, MAE: 0.001724 \n",
      "\n",
      "Epoch 29/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 29/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 29/200, Iteration 3/12, Loss: 0.0003\n",
      "Epoch 29/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 29/200, Iteration 5/12, Loss: 0.0003\n",
      "Epoch 29/200, Iteration 6/12, Loss: 0.0003\n",
      "Epoch 29/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 29/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 29/200, Iteration 9/12, Loss: 0.0003\n",
      "Epoch 29/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 29/200, Iteration 11/12, Loss: 0.0007\n",
      "Epoch 29/200, Iteration 12/12, Loss: 0.0004\n",
      "Epoch 29/200, Iteration 13/12, Loss: 0.0008\n",
      "Train Error: \n",
      " Accuracy: 99.75%, Avg loss: 0.000289, MRE: 0.009096, MAE: 0.001581 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000316, MRE: 0.007807, MAE: 0.001695 \n",
      "\n",
      "Epoch 30/200, Iteration 1/12, Loss: 0.0003\n",
      "Epoch 30/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 30/200, Iteration 3/12, Loss: 0.0007\n",
      "Epoch 30/200, Iteration 4/12, Loss: 0.0003\n",
      "Epoch 30/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 30/200, Iteration 6/12, Loss: 0.0003\n",
      "Epoch 30/200, Iteration 7/12, Loss: 0.0004\n",
      "Epoch 30/200, Iteration 8/12, Loss: 0.0004\n",
      "Epoch 30/200, Iteration 9/12, Loss: 0.0003\n",
      "Epoch 30/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 30/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 30/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 30/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000281, MRE: 0.008931, MAE: 0.001599 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000282, MRE: 0.007444, MAE: 0.001647 \n",
      "\n",
      "Epoch 31/200, Iteration 1/12, Loss: 0.0003\n",
      "Epoch 31/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 31/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 31/200, Iteration 4/12, Loss: 0.0004\n",
      "Epoch 31/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 31/200, Iteration 6/12, Loss: 0.0004\n",
      "Epoch 31/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 31/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 31/200, Iteration 9/12, Loss: 0.0003\n",
      "Epoch 31/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 31/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 31/200, Iteration 12/12, Loss: 0.0004\n",
      "Epoch 31/200, Iteration 13/12, Loss: 0.0005\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000250, MRE: 0.008665, MAE: 0.001465 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000290, MRE: 0.007596, MAE: 0.001618 \n",
      "\n",
      "Epoch 32/200, Iteration 1/12, Loss: 0.0003\n",
      "Epoch 32/200, Iteration 2/12, Loss: 0.0004\n",
      "Epoch 32/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 32/200, Iteration 4/12, Loss: 0.0003\n",
      "Epoch 32/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 32/200, Iteration 6/12, Loss: 0.0003\n",
      "Epoch 32/200, Iteration 7/12, Loss: 0.0003\n",
      "Epoch 32/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 32/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 32/200, Iteration 10/12, Loss: 0.0003\n",
      "Epoch 32/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 32/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 32/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 99.75%, Avg loss: 0.000246, MRE: 0.008495, MAE: 0.001440 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000221, MRE: 0.006396, MAE: 0.001470 \n",
      "\n",
      "Epoch 33/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 33/200, Iteration 2/12, Loss: 0.0004\n",
      "Epoch 33/200, Iteration 3/12, Loss: 0.0003\n",
      "Epoch 33/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 33/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 33/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 33/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 33/200, Iteration 8/12, Loss: 0.0003\n",
      "Epoch 33/200, Iteration 9/12, Loss: 0.0004\n",
      "Epoch 33/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 33/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 33/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 33/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000217, MRE: 0.008145, MAE: 0.001336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000246, MRE: 0.006981, MAE: 0.001487 \n",
      "\n",
      "Epoch 34/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 34/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 34/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 34/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 34/200, Iteration 5/12, Loss: 0.0003\n",
      "Epoch 34/200, Iteration 6/12, Loss: 0.0003\n",
      "Epoch 34/200, Iteration 7/12, Loss: 0.0003\n",
      "Epoch 34/200, Iteration 8/12, Loss: 0.0003\n",
      "Epoch 34/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 34/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 34/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 34/200, Iteration 12/12, Loss: 0.0003\n",
      "Epoch 34/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000208, MRE: 0.010064, MAE: 0.001297 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000214, MRE: 0.006375, MAE: 0.001393 \n",
      "\n",
      "Epoch 35/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 35/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 6/12, Loss: 0.0003\n",
      "Epoch 35/200, Iteration 7/12, Loss: 0.0004\n",
      "Epoch 35/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 35/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 35/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 35/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000193, MRE: 0.008008, MAE: 0.001274 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000189, MRE: 0.006245, MAE: 0.001326 \n",
      "\n",
      "Epoch 36/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 36/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 36/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 36/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 36/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 36/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 36/200, Iteration 7/12, Loss: 0.0003\n",
      "Epoch 36/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 36/200, Iteration 9/12, Loss: 0.0003\n",
      "Epoch 36/200, Iteration 10/12, Loss: 0.0003\n",
      "Epoch 36/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 36/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 36/200, Iteration 13/12, Loss: 0.0002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000184, MRE: 0.007766, MAE: 0.001222 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000185, MRE: 0.005891, MAE: 0.001288 \n",
      "\n",
      "Epoch 37/200, Iteration 1/12, Loss: 0.0004\n",
      "Epoch 37/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 37/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 37/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 37/200, Iteration 10/12, Loss: 0.0003\n",
      "Epoch 37/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 37/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 99.75%, Avg loss: 0.000206, MRE: 0.008271, MAE: 0.001291 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000174, MRE: 0.005781, MAE: 0.001299 \n",
      "\n",
      "Epoch 38/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 38/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 38/200, Iteration 3/12, Loss: 0.0004\n",
      "Epoch 38/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 38/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 38/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 38/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 38/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 38/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 38/200, Iteration 10/12, Loss: 0.0005\n",
      "Epoch 38/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 38/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 38/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000185, MRE: 0.007684, MAE: 0.001230 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000210, MRE: 0.006408, MAE: 0.001342 \n",
      "\n",
      "Epoch 39/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 39/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 39/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 39/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000191, MRE: 0.007866, MAE: 0.001258 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000221, MRE: 0.006464, MAE: 0.001363 \n",
      "\n",
      "Epoch 40/200, Iteration 1/12, Loss: 0.0003\n",
      "Epoch 40/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 40/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 40/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 40/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 40/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 40/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 40/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 40/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 40/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 40/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 40/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 40/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000168, MRE: 0.007578, MAE: 0.001163 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000158, MRE: 0.005653, MAE: 0.001210 \n",
      "\n",
      "Epoch 41/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 41/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 41/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 41/200, Iteration 4/12, Loss: 0.0003\n",
      "Epoch 41/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 41/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 41/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 41/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 41/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 41/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 41/200, Iteration 11/12, Loss: 0.0003\n",
      "Epoch 41/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 41/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000165, MRE: 0.007434, MAE: 0.001138 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000158, MRE: 0.005717, MAE: 0.001174 \n",
      "\n",
      "Epoch 42/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 42/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 42/200, Iteration 3/12, Loss: 0.0003\n",
      "Epoch 42/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 42/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 42/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 42/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 42/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 42/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 42/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 42/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 42/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 42/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000159, MRE: 0.007482, MAE: 0.001122 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000140, MRE: 0.005182, MAE: 0.001133 \n",
      "\n",
      "Epoch 43/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 43/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 43/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 43/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 43/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 43/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000150, MRE: 0.007353, MAE: 0.001081 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000145, MRE: 0.005251, MAE: 0.001139 \n",
      "\n",
      "Epoch 44/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 44/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 44/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 44/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 44/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 44/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 44/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 44/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 44/200, Iteration 9/12, Loss: 0.0004\n",
      "Epoch 44/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 44/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 44/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 44/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000213, MRE: 0.008181, MAE: 0.001280 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000251, MRE: 0.006905, MAE: 0.001423 \n",
      "\n",
      "Epoch 45/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 45/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 45/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 45/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 45/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 45/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 45/200, Iteration 7/12, Loss: 0.0003\n",
      "Epoch 45/200, Iteration 8/12, Loss: 0.0003\n",
      "Epoch 45/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 45/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 45/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 45/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 45/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000143, MRE: 0.007276, MAE: 0.001074 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000169, MRE: 0.005798, MAE: 0.001204 \n",
      "\n",
      "Epoch 46/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 46/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 46/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 46/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 46/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 46/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 46/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000134, MRE: 0.006895, MAE: 0.001044 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000138, MRE: 0.005277, MAE: 0.001121 \n",
      "\n",
      "Epoch 47/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 47/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 47/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 47/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 47/200, Iteration 11/12, Loss: 0.0003\n",
      "Epoch 47/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 47/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000133, MRE: 0.006960, MAE: 0.001035 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000126, MRE: 0.005018, MAE: 0.001101 \n",
      "\n",
      "Epoch 48/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 48/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 48/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 48/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 48/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 48/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 48/200, Iteration 13/12, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000145, MRE: 0.007069, MAE: 0.001101 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000123, MRE: 0.004918, MAE: 0.001109 \n",
      "\n",
      "Epoch 49/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 49/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 4/12, Loss: 0.0003\n",
      "Epoch 49/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 49/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 49/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 49/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000131, MRE: 0.006933, MAE: 0.001010 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000142, MRE: 0.005424, MAE: 0.001126 \n",
      "\n",
      "Epoch 50/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 50/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 50/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 50/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 50/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 50/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 50/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000126, MRE: 0.006751, MAE: 0.001012 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000137, MRE: 0.005352, MAE: 0.001094 \n",
      "\n",
      "Epoch 51/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 51/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 51/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 51/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 51/200, Iteration 13/12, Loss: 0.0004\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000124, MRE: 0.008459, MAE: 0.001009 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000133, MRE: 0.005114, MAE: 0.001100 \n",
      "\n",
      "Epoch 52/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 52/200, Iteration 3/12, Loss: 0.0002\n",
      "Epoch 52/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 52/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 10/12, Loss: 0.0002\n",
      "Epoch 52/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 52/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 52/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000121, MRE: 0.006743, MAE: 0.001008 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000111, MRE: 0.004691, MAE: 0.001039 \n",
      "\n",
      "Epoch 53/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 53/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 53/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 53/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 53/200, Iteration 13/12, Loss: 0.0003\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000131, MRE: 0.006760, MAE: 0.001022 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000107, MRE: 0.004601, MAE: 0.001040 \n",
      "\n",
      "Epoch 54/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 2/12, Loss: 0.0003\n",
      "Epoch 54/200, Iteration 3/12, Loss: 0.0003\n",
      "Epoch 54/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 54/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 54/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 54/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 99.88%, Avg loss: 0.000114, MRE: 0.006458, MAE: 0.000956 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000103, MRE: 0.004504, MAE: 0.000996 \n",
      "\n",
      "Epoch 55/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 5/12, Loss: 0.0002\n",
      "Epoch 55/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 8/12, Loss: 0.0002\n",
      "Epoch 55/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 55/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000119, MRE: 0.006582, MAE: 0.000959 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000108, MRE: 0.004501, MAE: 0.000999 \n",
      "\n",
      "Epoch 56/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 56/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 56/200, Iteration 12/12, Loss: 0.0002\n",
      "Epoch 56/200, Iteration 13/12, Loss: 0.0000\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000111, MRE: 0.006403, MAE: 0.000956 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000118, MRE: 0.004859, MAE: 0.001034 \n",
      "\n",
      "Epoch 57/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 57/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 57/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 57/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 57/200, Iteration 13/12, Loss: 0.0002\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000123, MRE: 0.006345, MAE: 0.000992 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000140, MRE: 0.005007, MAE: 0.001120 \n",
      "\n",
      "Epoch 58/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 58/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 58/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 58/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 58/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000121, MRE: 0.006649, MAE: 0.001008 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000109, MRE: 0.004836, MAE: 0.001096 \n",
      "\n",
      "Epoch 59/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 2/12, Loss: 0.0002\n",
      "Epoch 59/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 59/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 6/12, Loss: 0.0002\n",
      "Epoch 59/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 59/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000102, MRE: 0.006129, MAE: 0.000919 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000097, MRE: 0.004342, MAE: 0.000955 \n",
      "\n",
      "Epoch 60/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 60/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 60/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 60/200, Iteration 13/12, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000123, MRE: 0.006370, MAE: 0.000984 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000130, MRE: 0.004770, MAE: 0.001045 \n",
      "\n",
      "Epoch 61/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 7/12, Loss: 0.0002\n",
      "Epoch 61/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 9/12, Loss: 0.0002\n",
      "Epoch 61/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 61/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000158, MRE: 0.007002, MAE: 0.001096 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000166, MRE: 0.005535, MAE: 0.001166 \n",
      "\n",
      "Epoch 62/200, Iteration 1/12, Loss: 0.0002\n",
      "Epoch 62/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 11/12, Loss: 0.0002\n",
      "Epoch 62/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 62/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000104, MRE: 0.006164, MAE: 0.000946 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000122, MRE: 0.004963, MAE: 0.001081 \n",
      "\n",
      "Epoch 63/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 4/12, Loss: 0.0002\n",
      "Epoch 63/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 11/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 63/200, Iteration 13/12, Loss: 0.0001\n",
      "Train Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000094, MRE: 0.006001, MAE: 0.000892 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000099, MRE: 0.004710, MAE: 0.000980 \n",
      "\n",
      "Epoch 64/200, Iteration 1/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 2/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 3/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 4/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 5/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 6/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 7/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 8/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 9/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 10/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 11/12, Loss: 0.0003\n",
      "Epoch 64/200, Iteration 12/12, Loss: 0.0001\n",
      "Epoch 64/200, Iteration 13/12, Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200 #Iterationen über Datenset\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_MRE = []\n",
    "test_MRE = []\n",
    "train_MAE = []\n",
    "test_MAE = []\n",
    "\n",
    "#Runtime measurement\n",
    "train_time_start = time.process_time()\n",
    "#Optimierungsloop\n",
    "for epoch in range(num_epochs):\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "        \n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        \n",
    "#         print(X.shape)\n",
    "#         print(X.dtype)\n",
    "        \n",
    "        net.train() #Trainingmodus\n",
    "        \n",
    "        # forward\n",
    "        pred = net(X)  # Do the forward pass\n",
    "        loss = loss_fn_MSE(pred, y) # Calculate the loss\n",
    "        #loss = MRELoss(pred, y)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad() # Clear off the gradients from any past operation\n",
    "        loss.backward()       # Calculate the gradients with help of back propagation, updating weights and biases\n",
    "        \n",
    "        # adam step gradient descent\n",
    "        optimizer.step()      # Ask the optimizer to adjust the parameters based on the gradients  \n",
    "\n",
    "        print ('Epoch %d/%d, Iteration %d/%d, Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, batch+1, len(train_dataset)//batch_size, loss.item()))\n",
    "        \n",
    "    \n",
    "    #scheduler.step() # Reduzieren Learning Rate (falls step size erreicht)\n",
    "    net.eval() # Put the network into evaluation mode\n",
    "    \n",
    "    # Book keeping    \n",
    "    # What was our train accuracy?\n",
    "    tr_acc, tr_loss, tr_MRE, tr_MAE = check_accuracy(train_dataloader, net)\n",
    "    \n",
    "    #Record loss and accuracy\n",
    "    train_accuracy.append(tr_acc)\n",
    "    train_loss.append(tr_loss)\n",
    "    train_MRE.append(tr_MRE)\n",
    "    train_MAE.append(tr_MAE)\n",
    "    \n",
    "    scheduler.step(tr_loss) # LR scheduler step für reduceonPlateau\n",
    "    \n",
    "    # How did we do on the test set (the unseen set)\n",
    "    # Record the correct predictions for test data\n",
    "    t_acc, t_loss, t_MRE, t_MAE = check_accuracy(test_dataloader, net)\n",
    "    test_accuracy.append(t_acc)\n",
    "    test_loss.append(t_loss)\n",
    "    test_MRE.append(t_MRE)\n",
    "    test_MAE.append(t_MAE)\n",
    "\n",
    "train_time = time.process_time() - train_time_start\n",
    "print('Training time:',train_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9fb4a",
   "metadata": {},
   "source": [
    "#### Plots loss vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#fig.figsize=(12, 8)\n",
    "ax.semilogy(train_loss, label='train loss')\n",
    "ax.semilogy(test_loss, label='test loss')\n",
    "plt.title(\"Train and Test Loss\")\n",
    "ax.set(xlabel = '$Epochs$ / 1', ylabel = 'Loss / 1') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f9848",
   "metadata": {},
   "source": [
    "#### Parity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_H2_real_norm = []\n",
    "x_H2_pred_norm = []\n",
    "x_NH3_real_norm = []\n",
    "x_NH3_pred_norm = []\n",
    "for (X,y) in train_dataloader:\n",
    "    x_H2_real_norm = np.append(x_H2_real_norm, y[:,0].numpy())\n",
    "    x_NH3_real_norm = np.append(x_NH3_real_norm, y[:,1].numpy())\n",
    "    help_x_H2,help_x_NH3 = (net(X).detach().numpy()).T\n",
    "    x_H2_pred_norm = np.append(x_H2_pred_norm, help_x_H2)\n",
    "    x_NH3_pred_norm = np.append(x_NH3_pred_norm, help_x_NH3)\n",
    "\n",
    "x_H2_real_test_norm = []\n",
    "x_H2_pred_test_norm = []\n",
    "x_NH3_real_test_norm = []\n",
    "x_NH3_pred_test_norm = []\n",
    "for (X,y) in test_dataloader:\n",
    "    x_H2_real_test_norm = np.append(x_H2_real_test_norm, y[:,0].numpy())\n",
    "    x_NH3_real_test_norm = np.append(x_NH3_real_test_norm, y[:,1].numpy())\n",
    "    help_x_H2,help_x_NH3 = (net(X).detach().numpy()).T\n",
    "    x_H2_pred_test_norm = np.append(x_H2_pred_test_norm, help_x_H2)\n",
    "    x_NH3_pred_test_norm = np.append(x_NH3_pred_test_norm, help_x_NH3)\n",
    "\n",
    "x_H2_real = x_H2_real_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_pred = x_H2_pred_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_real = x_NH3_real_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_pred = x_NH3_pred_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "x_H2_real_test = x_H2_real_test_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_pred_test = x_H2_pred_test_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_real_test = x_NH3_real_test_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_pred_test = x_NH3_pred_test_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "print('Training Dataset: R^2(H2) =', r2(x_H2_real,x_H2_pred), ', R^2(NH3) =', r2(x_NH3_real,x_NH3_pred))\n",
    "print('Test Dataset: R^2(H2) =', r2(x_H2_real_test,x_H2_pred_test), ', R^2(NH3) =', r2(x_NH3_real_test,x_NH3_pred_test))\n",
    "print('Max Error Training: |x_H2 - x_H2,pred| =', max_error(x_H2_real, x_H2_pred), ', |x_NH3 - x_NH3,pred| =', max_error(x_NH3_real, x_NH3_pred))\n",
    "print('Max Error Test: |x_H2 - x_H2,pred| =', max_error(x_H2_real_test, x_H2_pred_test), ', |x_NH3 - x_NH3,pred| =', max_error(x_NH3_real_test, x_NH3_pred_test))\n",
    "\n",
    "# find the boundaries of X and Y values\n",
    "bounds = (0,1)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize =(10,10))\n",
    "\n",
    "# # Reset the limits\n",
    "# ax[0] = plt.gca()\n",
    "ax[0].set_xlim(bounds)\n",
    "ax[0].set_ylim(bounds)\n",
    "# Ensure the aspect ratio is square\n",
    "ax[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "ax[0].plot(x_H2_real, x_H2_pred, '.', color ='rebeccapurple', label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0].plot(x_NH3_real, x_NH3_pred, '.', color ='cornflowerblue', label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0].plot([0, 1], [0, 1], \"r-\",lw=2 ,transform=ax[0].transAxes)\n",
    "ax[0].plot([bounds[0],bounds[1]], [bounds[0] * 1.1, bounds[1] * 1.1], \"k--\") # Error line\n",
    "ax[0].plot([bounds[0],bounds[1]], [bounds[0] * 0.9, bounds[1] * 0.9], \"k--\") # Error line\n",
    "ax[0].text(0.7, 0.9, '+10%')\n",
    "ax[0].text(0.8, 0.65, '-10%')\n",
    "ax[0].set(xlabel = '$x$ / 1', ylabel = '$x\\mathregular{_{pred}}$ / 1')\n",
    "ax[0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0].set_title('Train Data')\n",
    "ax[0].legend()\n",
    "#ax[0].legend(['$\\\\mathregular{R^2}$ = ', r2(xi_real,xi_pred)], markerscale=0)\n",
    "\n",
    "# Reset the limits\n",
    "#ax[1] = plt.gca()\n",
    "ax[1].set_xlim(bounds)\n",
    "ax[1].set_ylim(bounds)\n",
    "# Ensure the aspect ratio is square\n",
    "ax[1].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "ax[1].plot(x_H2_real_test, x_H2_pred_test, '.', color ='rebeccapurple', label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[1].plot(x_NH3_real_test, x_NH3_pred_test, '.', color ='cornflowerblue', label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[1].plot([0, 1], [0, 1], \"r-\",lw=2 ,transform=ax[1].transAxes)\n",
    "ax[1].plot([bounds[0],bounds[1]], [bounds[0] * 1.1, bounds[1] * 1.1], \"k--\") # Error line\n",
    "ax[1].plot([bounds[0],bounds[1]], [bounds[0] * 0.9, bounds[1] * 0.9], \"k--\") # Error line\n",
    "ax[1].text(0.7, 0.9, '+10%')\n",
    "ax[1].text(0.8, 0.65, '-10%')\n",
    "ax[1].set(xlabel = '$x$ / 1', ylabel = '$x\\mathregular{_{pred}}$ / 1')\n",
    "ax[1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1].set_title('Test Data')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "#fig.suptitle(\"Parity Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.semilogy(x_H2_real, abs((x_H2_pred - x_H2_real) / x_H2_real), '.', label = 'MRE')\n",
    "ax.semilogy(x_H2_real, abs(x_H2_real-x_H2_pred), '.', label = 'MAE')\n",
    "ax.set(xlabel = '$x \\mathregular{_{H_2}}$ / 1', ylabel = 'Mistake')\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4390bab",
   "metadata": {},
   "source": [
    "#### Plot Fehler vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2)\n",
    "#fig.figsize=(12, 8)\n",
    "ax[0].semilogy(train_MRE, label='train MRE')\n",
    "ax[0].semilogy(test_MRE, label='test MRE')\n",
    "ax[0].set_title(\"Mean Relative Error\")\n",
    "ax[0].set(xlabel = '$Epochs$ / 1', ylabel = '$MRE$ / 1') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax[0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].semilogy(train_MAE, label='train MAE')\n",
    "ax[1].semilogy(test_MAE, label='test MAE')\n",
    "ax[1].set_title(\"Mean Absolute Error\")\n",
    "ax[1].set(xlabel = '$Epochs$ / 1', ylabel = '$MAE$ / mol') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax[1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbf7c2",
   "metadata": {},
   "source": [
    "#### Plot Loss vs Variable Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistake_H2 = []\n",
    "# mistake_NH3 = []\n",
    "x_H2_pred_norm = []\n",
    "x_NH3_pred_norm = []\n",
    "x_H2_real_norm = []\n",
    "x_NH3_real_norm = []\n",
    "param_T_norm = []\n",
    "param_p_norm = []\n",
    "param_x_H2_0_norm = []\n",
    "param_x_N2_0_norm = []\n",
    "param_x_NH3_0_norm = []\n",
    "for X,y in train_dataloader:\n",
    "#     help_mistake_H2, help_mistake_NH3 = (abs(y - net(X).detach().numpy())).T\n",
    "#     mistake_H2 = np.append(mistake_H2, help_mistake_H2)\n",
    "#     mistake_NH3 = np.append(mistake_NH3, help_mistake_NH3\n",
    "    help_pred = net(X).detach().numpy()\n",
    "    x_H2_pred_norm = np.append(x_H2_pred_norm, help_pred[:,0])\n",
    "    x_NH3_pred_norm = np.append(x_NH3_pred_norm, help_pred[:,1])\n",
    "    help_real = y.detach().numpy()\n",
    "    x_H2_real_norm = np.append(x_H2_real_norm, help_real[:,0])\n",
    "    x_NH3_real_norm = np.append(x_NH3_real_norm, help_real[:,1])\n",
    "    param_T_norm = np.append(param_T_norm, X[:,0])\n",
    "    param_p_norm = np.append(param_p_norm, X[:,1])\n",
    "    param_x_H2_0_norm = np.append(param_x_H2_0_norm, X[:,2])\n",
    "    param_x_N2_0_norm = np.append(param_x_N2_0_norm, X[:,3])\n",
    "    param_x_NH3_0_norm = np.append(param_x_NH3_0_norm, X[:,4])\n",
    "\n",
    "# print('x_H2:', x_H2_real_norm) #, x_H2_real_norm.dtype())\n",
    "# print('x_H2_pred:', x_H2_pred_norm)\n",
    "x_H2_pred = x_H2_pred_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_real = x_H2_real_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_pred = x_NH3_pred_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_real = x_NH3_real_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "mistake_H2 = abs(x_H2_real - x_H2_pred)\n",
    "mistake_NH3 = abs(x_NH3_real - x_NH3_pred)\n",
    "\n",
    "param_T = param_T_norm * std_in[0].numpy() + mean_in[0].numpy()\n",
    "param_p = param_p_norm * std_in[1].numpy() + mean_in[1].numpy()\n",
    "param_x_H2_0 = param_x_H2_0_norm * std_in[2].numpy() + mean_in[2].numpy()\n",
    "param_x_N2_0 = param_x_N2_0_norm * std_in[3].numpy() + mean_in[3].numpy()\n",
    "param_x_NH3_0 = param_x_NH3_0_norm * std_in[4].numpy() + mean_in[4].numpy()\n",
    "\n",
    "# print('T:', param_T[0])\n",
    "# print(len(param_T))\n",
    "# print(param_T[0])\n",
    "\n",
    "fig,ax = plt.subplots(2,2, figsize = (10, 7)) #gridspec_kw={'width_ratios': [1,1,1,1]})\n",
    "\n",
    "ax[0,0].semilogy(param_T, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0,0].semilogy(param_T, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0,0].set(xlabel = '$T$ / K', ylabel = '|$x\\mathregular{_i} - x\\mathregular{_{i,pred}}$| / mol')\n",
    "ax[0,0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].semilogy(param_p, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0,1].semilogy(param_p, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0,1].set(xlabel = '$p$ / bar', ylabel = '|$x\\mathregular{_i} - x\\mathregular{_{i,pred}}$| / mol')\n",
    "ax[0,1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,0].semilogy(param_x_H2_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2, 0}}$')\n",
    "ax[1,0].semilogy(param_x_N2_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{N_2, 0}}$')\n",
    "ax[1,0].semilogy(param_x_NH3_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{NH_3, 0}}$')\n",
    "ax[1,0].set(xlabel = '$x\\mathregular{_{i,0}}$ / 1', ylabel = '|$x\\mathregular{_{H_2}} - x\\mathregular{_{H_2,pred}}$| / mol')\n",
    "ax[1,0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1,0].set(xlim = (0,1))\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].semilogy(param_x_H2_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{H_2, 0}}$')\n",
    "ax[1,1].semilogy(param_x_N2_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{N_2, 0}}$')\n",
    "ax[1,1].semilogy(param_x_NH3_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3, 0}}$')\n",
    "ax[1,1].set(xlabel = '$x\\mathregular{_{i,0}}$ / 1', ylabel = '|$x\\mathregular{_{NH_3}} - x\\mathregular{_{NH_3,pred}}$| / mol')\n",
    "ax[1,1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1,1].set(xlim = (0,1))\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b5833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#fig.figsize=(12, 8)\n",
    "ax.plot(train_accuracy, label='train accuracy')\n",
    "ax.plot(test_accuracy, label='test accuracy')\n",
    "plt.title(\"Train and Test Accuracy\")\n",
    "ax.set(xlabel = '$Epochs$ / 1', ylabel = '$Accuracy$ / %') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff0471",
   "metadata": {},
   "source": [
    "#### Laufzeit Gleichgewichtsvorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e60ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred_time = time.process_time()\n",
    "for X, y in train_dataloader:\n",
    "            pred = net(X)\n",
    "pred_time = (time.process_time() - start_pred_time)\n",
    "print('Prediction time:', pred_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418caa55",
   "metadata": {},
   "source": [
    "#### Debugging Hilfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen aller Input X und Output y Daten\n",
    "for (X,y) in train_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print(net(X))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241eab8",
   "metadata": {},
   "source": [
    "#### Einblick in Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b043958",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters()) # zeigt weights, biases, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2,5))\n",
    "print(X)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lnorm = nn.LayerNorm(5)\n",
    "Bnorm = nn.BatchNorm1d(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X,y) in train_dataloader:\n",
    "    print(X)\n",
    "    #print(y.reshape((-1,1)))\n",
    "    print(Bnorm(X).mean(dim=0))\n",
    "    print(Bnorm(X))\n",
    "    print(Lnorm(X))\n",
    "    #print((Lnorm(X.permute(0,2,1))).permute(0,2,1))\n",
    "    print(Lnorm(X).mean(dim=0))\n",
    "    print(Lnorm(X).mean(dim=1))\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b867dae",
   "metadata": {},
   "source": [
    "#### Histogramme Verteilung von $xi$ und $x{_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(xi)\n",
    "plt.hist(x_0[:,0],bins=100)\n",
    "plt.hist(x_0[:,1],bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c529b97",
   "metadata": {},
   "source": [
    "#### Speichern des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(),'data/models/ann_005_024.pth')\n",
    "# np.savez('data/models/params_005_024.npz', mean_in = mean_in, std_in = std_in, mean_out = mean_out, std_out = std_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
