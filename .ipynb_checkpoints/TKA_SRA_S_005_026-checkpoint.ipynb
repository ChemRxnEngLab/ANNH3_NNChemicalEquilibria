{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54dc0a0",
   "metadata": {},
   "source": [
    "# Architektur Neuronales Netz, Output x_H2 und x_NH3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250c18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe / Bibliotheken\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import normalize as norm\n",
    "from torch import log10\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import max_error\n",
    "# from sklearn.metrics import mean_squared_error as MSE\n",
    "# from sklearn.metrics import mean_absolute_error as MAE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9ebf3",
   "metadata": {},
   "source": [
    "#### Default Datentyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68df48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e5067",
   "metadata": {},
   "source": [
    "#### Erzeugnung des Moduls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bffc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    #Initalisierung der Netzwerk layers\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "    \n",
    "        super().__init__() #Referenz zur Base Class (nn.Module)\n",
    "        #Kaskade der Layer\n",
    "        self.linear_afunc_stack = nn.Sequential(\n",
    "            #nn.BatchNorm1d(input_size), # Normalisierung, damit Inputdaten gleiche Größenordnung haben\n",
    "            nn.Linear(input_size, hidden1_size), #Lineare Transformation mit gespeicherten weights und biases\n",
    "            nn.GELU(), #Nicht lineare Aktivierungsfunktion um komplexe nichtlineare Zusammenhänge abzubilden\n",
    "            nn.Linear(hidden1_size, hidden2_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden2_size, output_size),\n",
    "        )\n",
    "\n",
    "    #Implementierung der Operationen auf Input Daten\n",
    "    def forward(self, x):\n",
    "        out = self.linear_afunc_stack(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9ae53",
   "metadata": {},
   "source": [
    "#### Ausgabe Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0ecc2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden3_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mNeuralNetwork.__init__\u001b[1;34m(self, input_size, hidden1_size, hidden2_size, output_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m() \u001b[38;5;66;03m#Referenz zur Base Class (nn.Module)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Kaskade der Layer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_afunc_stack \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#nn.BatchNorm1d(input_size), # Normalisierung, damit Inputdaten gleiche Größenordnung haben\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(input_size, hidden1_size), \u001b[38;5;66;03m#Lineare Transformation mit gespeicherten weights und biases\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     nn\u001b[38;5;241m.\u001b[39mGELU(), \u001b[38;5;66;03m#Nicht lineare Aktivierungsfunktion um komplexe nichtlineare Zusammenhänge abzubilden\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(hidden1_size, hidden2_size),\n\u001b[0;32m     13\u001b[0m     nn\u001b[38;5;241m.\u001b[39mGELU(),\n\u001b[1;32m---> 14\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(hidden2_size, \u001b[43mhidden3_size\u001b[49m),\n\u001b[0;32m     15\u001b[0m     nn\u001b[38;5;241m.\u001b[39mGELU(),\n\u001b[0;32m     16\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(hidden3_size, output_size),\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden3_size' is not defined"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(5, 200, 200, 2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1d6ae",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 #Zahl der Datenpaare die vor einem erneuten Update der Parameter ins Netzt gegeben werden\n",
    "eq_data_file = Path.cwd() / 'data' / 'eq_dataset_x.npz' #Import der GGW Daten\n",
    "\n",
    "res = np.load(eq_data_file)\n",
    "\n",
    "# Bei Speicherung wurden Daten als T, p, x_0 und xi gespeichert\n",
    "# Inputs T, p, x_0[H2,N2,NH3]\n",
    "# Outputs x[H2,N2,NH3]\n",
    "# Umwandlen der np.arrays in torch.tensors zur besseren Arbeit mit PyTorch\n",
    "T = torch.tensor(res['T'])\n",
    "p = torch.tensor(res['p'])\n",
    "x_0 = torch.tensor(res['x_0'])\n",
    "x = torch.tensor(res['x'])\n",
    "\n",
    "#Anpassen der Daten auf gleiche Größenordnung\n",
    "#T = log10(T)\n",
    "# T = T / 850\n",
    "# p = p / 1000\n",
    "\n",
    "\n",
    "# print(T.dtype)\n",
    "# print(xi.dtype)\n",
    "\n",
    "x_input = torch.stack((T, p ,x_0[:,0],x_0[:,1],x_0[:,2]),1)\n",
    "y_output = torch.stack((x[:,0], x[:,2]), 1) # [H2, NH3], dritter Stoffmengenanteil ergibt sich den anderen\n",
    "#print(x_input.size())\n",
    "# print(xi.size())\n",
    "\n",
    "# Split des Datensatzes in Trainings und Testdaten\n",
    "split = 0.8 # Anteil Trainingsdaten\n",
    "\n",
    "x_input_train = x_input[:int(split * len(x_input)), :]\n",
    "y_output_train = y_output[:int(split * len(y_output)), :]\n",
    "x_input_test = x_input[int(split * len(x_input)):, :]\n",
    "y_output_test = y_output[int(split * len(y_output)):, :]\n",
    "\n",
    "# Preprocessing Normalisierung der Daten\n",
    "mean_in = torch.mean(x_input_train,0) # Mittelwert\n",
    "std_in = torch.std(x_input_train,0) # Standardabweichung\n",
    "mean_out = torch.mean(y_output_train,0)\n",
    "std_out = torch.std(y_output_train,0)\n",
    "\n",
    "x_input_train_norm = (x_input_train - mean_in) / std_in\n",
    "y_output_train_norm = (y_output_train - mean_out) / std_out\n",
    "\n",
    "x_input_test_norm = (x_input_test - mean_in) / std_in\n",
    "y_output_test_norm = (y_output_test - mean_out) / std_out\n",
    "\n",
    "# print(x_input_train_norm)\n",
    "# print(torch.mean(x_input_train_norm[:,0]))\n",
    "\n",
    "# Tensoren zu einem großen Set gruppieren\n",
    "train_dataset = TensorDataset(x_input_train_norm, y_output_train_norm)\n",
    "test_dataset = TensorDataset(x_input_test_norm, y_output_test_norm)\n",
    "    \n",
    "# # Split in Trainings und Test Set\n",
    "# train_dataset, test_dataset = random_split(dataset, \n",
    "#                                            [int(0.8*len(dataset)), int(0.2*len(dataset))], # splitting 80/20\n",
    "#                                            generator = torch.Generator().manual_seed(42)) # Festlegung seed zur Reproduktivität\n",
    "\n",
    "# Erzeugen der DataLoader zur Arbeit mit Daten\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True) # shuffle batches zur Reduzierung von overfitting\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e9841",
   "metadata": {},
   "source": [
    "#### Generierung Netzwerk, Festlegung von loss Funktion und Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung aNN\n",
    "net = NeuralNetwork(5, 200, 200, 2)\n",
    "\n",
    "# Loss Funktion; gibt Fehler an\n",
    "loss_fn_MSE = nn.MSELoss()\n",
    "loss_fn_L1 = nn.L1Loss()\n",
    "\n",
    "#Definition custom loss Funktion, MRE\n",
    "def MRELoss(outputs, targets):\n",
    "    \n",
    "    loss = torch.mean(abs((outputs - targets) / targets))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "#Optimizer\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "#scheduler = StepLR(optimizer, step_size = 30, gamma = 0.1)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[30, 70, 100], gamma = 0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 10, threshold = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-6\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b61b7",
   "metadata": {},
   "source": [
    "#### Funktion zur Bestimmung der Genauigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4480b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, net):\n",
    "    \n",
    "    loss = 0\n",
    "    MRE = 0\n",
    "    MAE = 0\n",
    "    train_correct = 0\n",
    "    train_total = len(loader.dataset)\n",
    "    num_batches = len(loader) \n",
    "    #train_total = 0\n",
    "    \n",
    "    net.eval() # Put network in evaluation mode\n",
    "    \n",
    "    if loader == train_dataloader:\n",
    "        dataset = \"Train\"\n",
    "    else:\n",
    "        dataset = \"Test\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            pred = net(X)\n",
    "           \n",
    "            #loss += MRELoss(pred, y).item()\n",
    "            loss += loss_fn_MSE(pred, y) # Calculate the loss\n",
    "            MRE += MRELoss(pred * std_out + mean_out, y * std_out + mean_out)\n",
    "            MAE += loss_fn_L1(pred * std_out + mean_out, y * std_out + mean_out)\n",
    "            \n",
    "            # Record the correct predictions for training data\n",
    "            #_, predictions = torch.max(pred.data, 1)\n",
    "            for i in range(len(pred)):\n",
    "                if ((pred[i,0] * std_out[0] + mean_out[0]) - (y[i,0] * std_out[0] + mean_out[0]) and (pred[i,1] * std_out[1] + mean_out[1]) - (y[i,1] * std_out[1] + mean_out[1])) <= 0.01:\n",
    "                    train_correct += 1\n",
    "            #train_correct += (abs(pred.argmax(1) - y) <= 0.01).sum().item()\n",
    "            #train_correct += (abs(predictions - y.data) <= 0.01).sum()\n",
    "            #train_total += predictions.size(0)\n",
    "            \n",
    "        # Genauigkeit berechnen\n",
    "        acc = float(train_correct) / float(train_total) * 100\n",
    "        acc = round(acc, 2)\n",
    "        \n",
    "        loss /= num_batches\n",
    "        MRE /= num_batches\n",
    "        MAE /= num_batches\n",
    "\n",
    "        print(f\"{dataset} Error: \\n Accuracy: {acc}%, Avg loss: {loss:>8f}, MRE: {MRE:>8f}, MAE: {MAE:>8f} \\n\")\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    return acc, loss, MRE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd049ed",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771789d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200 #Iterationen über Datenset\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_MRE = []\n",
    "test_MRE = []\n",
    "train_MAE = []\n",
    "test_MAE = []\n",
    "\n",
    "#Runtime measurement\n",
    "train_time_start = time.process_time()\n",
    "#Optimierungsloop\n",
    "for epoch in range(num_epochs):\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "        \n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        \n",
    "#         print(X.shape)\n",
    "#         print(X.dtype)\n",
    "        \n",
    "        net.train() #Trainingmodus\n",
    "        \n",
    "        # forward\n",
    "        pred = net(X)  # Do the forward pass\n",
    "        loss = loss_fn_MSE(pred, y) # Calculate the loss\n",
    "        #loss = MRELoss(pred, y)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad() # Clear off the gradients from any past operation\n",
    "        loss.backward()       # Calculate the gradients with help of back propagation, updating weights and biases\n",
    "        \n",
    "        # adam step gradient descent\n",
    "        optimizer.step()      # Ask the optimizer to adjust the parameters based on the gradients  \n",
    "\n",
    "        print ('Epoch %d/%d, Iteration %d/%d, Loss: %.4f' \n",
    "               %(epoch+1, num_epochs, batch+1, len(train_dataset)//batch_size, loss.item()))\n",
    "        \n",
    "    \n",
    "    #scheduler.step() # Reduzieren Learning Rate (falls step size erreicht)\n",
    "    net.eval() # Put the network into evaluation mode\n",
    "    \n",
    "    # Book keeping    \n",
    "    # What was our train accuracy?\n",
    "    tr_acc, tr_loss, tr_MRE, tr_MAE = check_accuracy(train_dataloader, net)\n",
    "    \n",
    "    #Record loss and accuracy\n",
    "    train_accuracy.append(tr_acc)\n",
    "    train_loss.append(tr_loss)\n",
    "    train_MRE.append(tr_MRE)\n",
    "    train_MAE.append(tr_MAE)\n",
    "    \n",
    "    scheduler.step(tr_loss) # LR scheduler step für reduceonPlateau\n",
    "    \n",
    "    # How did we do on the test set (the unseen set)\n",
    "    # Record the correct predictions for test data\n",
    "    t_acc, t_loss, t_MRE, t_MAE = check_accuracy(test_dataloader, net)\n",
    "    test_accuracy.append(t_acc)\n",
    "    test_loss.append(t_loss)\n",
    "    test_MRE.append(t_MRE)\n",
    "    test_MAE.append(t_MAE)\n",
    "\n",
    "train_time = time.process_time() - train_time_start\n",
    "print('Training time:',train_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9fb4a",
   "metadata": {},
   "source": [
    "#### Plots loss vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#fig.figsize=(12, 8)\n",
    "ax.semilogy(train_loss, label='train loss')\n",
    "ax.semilogy(test_loss, label='test loss')\n",
    "plt.title(\"Train and Test Loss\")\n",
    "ax.set(xlabel = '$Epochs$ / 1', ylabel = 'Loss / 1') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f9848",
   "metadata": {},
   "source": [
    "#### Parity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_H2_real_norm = []\n",
    "x_H2_pred_norm = []\n",
    "x_NH3_real_norm = []\n",
    "x_NH3_pred_norm = []\n",
    "for (X,y) in train_dataloader:\n",
    "    x_H2_real_norm = np.append(x_H2_real_norm, y[:,0].numpy())\n",
    "    x_NH3_real_norm = np.append(x_NH3_real_norm, y[:,1].numpy())\n",
    "    help_x_H2,help_x_NH3 = (net(X).detach().numpy()).T\n",
    "    x_H2_pred_norm = np.append(x_H2_pred_norm, help_x_H2)\n",
    "    x_NH3_pred_norm = np.append(x_NH3_pred_norm, help_x_NH3)\n",
    "\n",
    "x_H2_real_test_norm = []\n",
    "x_H2_pred_test_norm = []\n",
    "x_NH3_real_test_norm = []\n",
    "x_NH3_pred_test_norm = []\n",
    "for (X,y) in test_dataloader:\n",
    "    x_H2_real_test_norm = np.append(x_H2_real_test_norm, y[:,0].numpy())\n",
    "    x_NH3_real_test_norm = np.append(x_NH3_real_test_norm, y[:,1].numpy())\n",
    "    help_x_H2,help_x_NH3 = (net(X).detach().numpy()).T\n",
    "    x_H2_pred_test_norm = np.append(x_H2_pred_test_norm, help_x_H2)\n",
    "    x_NH3_pred_test_norm = np.append(x_NH3_pred_test_norm, help_x_NH3)\n",
    "\n",
    "x_H2_real = x_H2_real_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_pred = x_H2_pred_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_real = x_NH3_real_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_pred = x_NH3_pred_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "x_H2_real_test = x_H2_real_test_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_pred_test = x_H2_pred_test_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_real_test = x_NH3_real_test_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_pred_test = x_NH3_pred_test_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "print('Training Dataset: R^2(H2) =', r2(x_H2_real,x_H2_pred), ', R^2(NH3) =', r2(x_NH3_real,x_NH3_pred))\n",
    "print('Test Dataset: R^2(H2) =', r2(x_H2_real_test,x_H2_pred_test), ', R^2(NH3) =', r2(x_NH3_real_test,x_NH3_pred_test))\n",
    "print('Max Error Training: |x_H2 - x_H2,pred| =', max_error(x_H2_real, x_H2_pred), ', |x_NH3 - x_NH3,pred| =', max_error(x_NH3_real, x_NH3_pred))\n",
    "print('Max Error Test: |x_H2 - x_H2,pred| =', max_error(x_H2_real_test, x_H2_pred_test), ', |x_NH3 - x_NH3,pred| =', max_error(x_NH3_real_test, x_NH3_pred_test))\n",
    "\n",
    "# find the boundaries of X and Y values\n",
    "bounds = (0,1)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize =(10,10))\n",
    "\n",
    "# # Reset the limits\n",
    "# ax[0] = plt.gca()\n",
    "ax[0].set_xlim(bounds)\n",
    "ax[0].set_ylim(bounds)\n",
    "# Ensure the aspect ratio is square\n",
    "ax[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "ax[0].plot(x_H2_real, x_H2_pred, '.', color ='rebeccapurple', label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0].plot(x_NH3_real, x_NH3_pred, '.', color ='cornflowerblue', label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0].plot([0, 1], [0, 1], \"r-\",lw=2 ,transform=ax[0].transAxes)\n",
    "ax[0].plot([bounds[0],bounds[1]], [bounds[0] * 1.1, bounds[1] * 1.1], \"k--\") # Error line\n",
    "ax[0].plot([bounds[0],bounds[1]], [bounds[0] * 0.9, bounds[1] * 0.9], \"k--\") # Error line\n",
    "ax[0].text(0.7, 0.9, '+10%')\n",
    "ax[0].text(0.8, 0.65, '-10%')\n",
    "ax[0].set(xlabel = '$x$ / 1', ylabel = '$x\\mathregular{_{pred}}$ / 1')\n",
    "ax[0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0].set_title('Train Data')\n",
    "ax[0].legend()\n",
    "#ax[0].legend(['$\\\\mathregular{R^2}$ = ', r2(xi_real,xi_pred)], markerscale=0)\n",
    "\n",
    "# Reset the limits\n",
    "#ax[1] = plt.gca()\n",
    "ax[1].set_xlim(bounds)\n",
    "ax[1].set_ylim(bounds)\n",
    "# Ensure the aspect ratio is square\n",
    "ax[1].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "ax[1].plot(x_H2_real_test, x_H2_pred_test, '.', color ='rebeccapurple', label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[1].plot(x_NH3_real_test, x_NH3_pred_test, '.', color ='cornflowerblue', label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[1].plot([0, 1], [0, 1], \"r-\",lw=2 ,transform=ax[1].transAxes)\n",
    "ax[1].plot([bounds[0],bounds[1]], [bounds[0] * 1.1, bounds[1] * 1.1], \"k--\") # Error line\n",
    "ax[1].plot([bounds[0],bounds[1]], [bounds[0] * 0.9, bounds[1] * 0.9], \"k--\") # Error line\n",
    "ax[1].text(0.7, 0.9, '+10%')\n",
    "ax[1].text(0.8, 0.65, '-10%')\n",
    "ax[1].set(xlabel = '$x$ / 1', ylabel = '$x\\mathregular{_{pred}}$ / 1')\n",
    "ax[1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1].set_title('Test Data')\n",
    "ax[1].legend()\n",
    "\n",
    "\n",
    "#plt.legend()\n",
    "#fig.suptitle(\"Parity Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.semilogy(x_H2_real, abs((x_H2_pred - x_H2_real) / x_H2_real), '.', label = 'MRE')\n",
    "ax.semilogy(x_H2_real, abs(x_H2_real-x_H2_pred), '.', label = 'MAE')\n",
    "ax.set(xlabel = '$x \\mathregular{_{H_2}}$ / 1', ylabel = 'Mistake')\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4390bab",
   "metadata": {},
   "source": [
    "#### Plot Fehler vs Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2)\n",
    "#fig.figsize=(12, 8)\n",
    "ax[0].semilogy(train_MRE, label='train MRE')\n",
    "ax[0].semilogy(test_MRE, label='test MRE')\n",
    "ax[0].set_title(\"Mean Relative Error\")\n",
    "ax[0].set(xlabel = '$Epochs$ / 1', ylabel = '$MRE$ / 1') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax[0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].semilogy(train_MAE, label='train MAE')\n",
    "ax[1].semilogy(test_MAE, label='test MAE')\n",
    "ax[1].set_title(\"Mean Absolute Error\")\n",
    "ax[1].set(xlabel = '$Epochs$ / 1', ylabel = '$MAE$ / mol') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax[1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbf7c2",
   "metadata": {},
   "source": [
    "#### Plot Loss vs Variable Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistake_H2 = []\n",
    "# mistake_NH3 = []\n",
    "x_H2_pred_norm = []\n",
    "x_NH3_pred_norm = []\n",
    "x_H2_real_norm = []\n",
    "x_NH3_real_norm = []\n",
    "param_T_norm = []\n",
    "param_p_norm = []\n",
    "param_x_H2_0_norm = []\n",
    "param_x_N2_0_norm = []\n",
    "param_x_NH3_0_norm = []\n",
    "for X,y in train_dataloader:\n",
    "#     help_mistake_H2, help_mistake_NH3 = (abs(y - net(X).detach().numpy())).T\n",
    "#     mistake_H2 = np.append(mistake_H2, help_mistake_H2)\n",
    "#     mistake_NH3 = np.append(mistake_NH3, help_mistake_NH3\n",
    "    help_pred = net(X).detach().numpy()\n",
    "    x_H2_pred_norm = np.append(x_H2_pred_norm, help_pred[:,0])\n",
    "    x_NH3_pred_norm = np.append(x_NH3_pred_norm, help_pred[:,1])\n",
    "    help_real = y.detach().numpy()\n",
    "    x_H2_real_norm = np.append(x_H2_real_norm, help_real[:,0])\n",
    "    x_NH3_real_norm = np.append(x_NH3_real_norm, help_real[:,1])\n",
    "    param_T_norm = np.append(param_T_norm, X[:,0])\n",
    "    param_p_norm = np.append(param_p_norm, X[:,1])\n",
    "    param_x_H2_0_norm = np.append(param_x_H2_0_norm, X[:,2])\n",
    "    param_x_N2_0_norm = np.append(param_x_N2_0_norm, X[:,3])\n",
    "    param_x_NH3_0_norm = np.append(param_x_NH3_0_norm, X[:,4])\n",
    "\n",
    "# print('x_H2:', x_H2_real_norm) #, x_H2_real_norm.dtype())\n",
    "# print('x_H2_pred:', x_H2_pred_norm)\n",
    "x_H2_pred = x_H2_pred_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_H2_real = x_H2_real_norm * std_out[0].numpy() + mean_out[0].numpy()\n",
    "x_NH3_pred = x_NH3_pred_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "x_NH3_real = x_NH3_real_norm * std_out[1].numpy() + mean_out[1].numpy()\n",
    "\n",
    "mistake_H2 = abs(x_H2_real - x_H2_pred)\n",
    "mistake_NH3 = abs(x_NH3_real - x_NH3_pred)\n",
    "\n",
    "param_T = param_T_norm * std_in[0].numpy() + mean_in[0].numpy()\n",
    "param_p = param_p_norm * std_in[1].numpy() + mean_in[1].numpy()\n",
    "param_x_H2_0 = param_x_H2_0_norm * std_in[2].numpy() + mean_in[2].numpy()\n",
    "param_x_N2_0 = param_x_N2_0_norm * std_in[3].numpy() + mean_in[3].numpy()\n",
    "param_x_NH3_0 = param_x_NH3_0_norm * std_in[4].numpy() + mean_in[4].numpy()\n",
    "\n",
    "# print('T:', param_T[0])\n",
    "# print(len(param_T))\n",
    "# print(param_T[0])\n",
    "\n",
    "fig,ax = plt.subplots(2,2, figsize = (10, 7)) #gridspec_kw={'width_ratios': [1,1,1,1]})\n",
    "\n",
    "ax[0,0].semilogy(param_T, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0,0].semilogy(param_T, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0,0].set(xlabel = '$T$ / K', ylabel = '|$x\\mathregular{_i} - x\\mathregular{_{i,pred}}$| / mol')\n",
    "ax[0,0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].semilogy(param_p, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2}}$')\n",
    "ax[0,1].semilogy(param_p, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3}}$')\n",
    "ax[0,1].set(xlabel = '$p$ / bar', ylabel = '|$x\\mathregular{_i} - x\\mathregular{_{i,pred}}$| / mol')\n",
    "ax[0,1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,0].semilogy(param_x_H2_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{H_2, 0}}$')\n",
    "ax[1,0].semilogy(param_x_N2_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{N_2, 0}}$')\n",
    "ax[1,0].semilogy(param_x_NH3_0, mistake_H2, '.', markersize = 2, label = '$x\\mathregular{_{NH_3, 0}}$')\n",
    "ax[1,0].set(xlabel = '$x\\mathregular{_{i,0}}$ / 1', ylabel = '|$x\\mathregular{_{H_2}} - x\\mathregular{_{H_2,pred}}$| / mol')\n",
    "ax[1,0].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1,0].set(xlim = (0,1))\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].semilogy(param_x_H2_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{H_2, 0}}$')\n",
    "ax[1,1].semilogy(param_x_N2_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{N_2, 0}}$')\n",
    "ax[1,1].semilogy(param_x_NH3_0, mistake_NH3, '.', markersize = 2, label = '$x\\mathregular{_{NH_3, 0}}$')\n",
    "ax[1,1].set(xlabel = '$x\\mathregular{_{i,0}}$ / 1', ylabel = '|$x\\mathregular{_{NH_3}} - x\\mathregular{_{NH_3,pred}}$| / mol')\n",
    "ax[1,1].tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "ax[1,1].set(xlim = (0,1))\n",
    "ax[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b5833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "#fig.figsize=(12, 8)\n",
    "ax.plot(train_accuracy, label='train accuracy')\n",
    "ax.plot(test_accuracy, label='test accuracy')\n",
    "plt.title(\"Train and Test Accuracy\")\n",
    "ax.set(xlabel = '$Epochs$ / 1', ylabel = '$Accuracy$ / %') #Beschriftung Achsen; Kursiv durch $$; Index durch _{}\n",
    "ax.tick_params(direction = 'in') #, length = 20, width = 3)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff0471",
   "metadata": {},
   "source": [
    "#### Laufzeit Gleichgewichtsvorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e60ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pred_time = time.process_time()\n",
    "for X, y in train_dataloader:\n",
    "            pred = net(X)\n",
    "pred_time = (time.process_time() - start_pred_time)\n",
    "print('Prediction time:', pred_time, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418caa55",
   "metadata": {},
   "source": [
    "#### Debugging Hilfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen aller Input X und Output y Daten\n",
    "for (X,y) in train_dataloader:\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print(net(X))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241eab8",
   "metadata": {},
   "source": [
    "#### Einblick in Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b043958",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters()) # zeigt weights, biases, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2,5))\n",
    "print(X)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lnorm = nn.LayerNorm(5)\n",
    "Bnorm = nn.BatchNorm1d(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X,y) in train_dataloader:\n",
    "    print(X)\n",
    "    #print(y.reshape((-1,1)))\n",
    "    print(Bnorm(X).mean(dim=0))\n",
    "    print(Bnorm(X))\n",
    "    print(Lnorm(X))\n",
    "    #print((Lnorm(X.permute(0,2,1))).permute(0,2,1))\n",
    "    print(Lnorm(X).mean(dim=0))\n",
    "    print(Lnorm(X).mean(dim=1))\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b867dae",
   "metadata": {},
   "source": [
    "#### Histogramme Verteilung von $xi$ und $x{_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(xi)\n",
    "plt.hist(x_0[:,0],bins=100)\n",
    "plt.hist(x_0[:,1],bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c529b97",
   "metadata": {},
   "source": [
    "#### Speichern des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(),'data/models/ann_005_022.pth')\n",
    "# np.savez('data/models/params_005_022.npz', mean_in = mean_in, std_in = std_in, mean_out = mean_out, std_out = std_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
